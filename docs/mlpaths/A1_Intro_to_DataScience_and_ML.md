

# Self-Directed Learning Module: Introduction to Data Science and Machine Learning ðŸš€ðŸ“Š

<img src="https://images.unsplash.com/photo-1548892716-ccc4ff70ca8c?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" width=840>

(Image credit: [Jason Leung](https://unsplash.com/@ninjason). Unsplash.com)


***

Welcome to your introductory journey into the exciting fields of **Data Science and Machine Learning!**

This module is designed for self-paced learning, empowering you to explore foundational concepts and develop practical Python skills. You'll be encouraged to use AI tools to enhance your understanding and troubleshoot challenges along the way.


***

## 1. General Learning Objective

Upon completing this module, you will be able to define core data science and machine learning concepts, perform fundamental data manipulation and visualization tasks using Python, and independently utilize open-source Large Language Models (LLMs) and related AI tools to support your coding, problem-solving, and learning process in a data science context.

---


## 2. Topic Overview

[Data Science](https://en.wikipedia.org/wiki/Data_science) is an interdisciplinary field focused on extracting knowledge and insights from data. [Machine Learning (ML)](https://en.wikipedia.org/wiki/Machine_learning), a key component of [Artificial Intelligence (AI)](https://en.wikipedia.org/wiki/Artificial_intelligence), enables systems to learn from data to make decisions or predictions.

This module introduces the typical data science workflowâ€”from asking the right questions and gathering data to building models and communicating results. You'll see how [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) is the go-to language for these tasks and how AI tools can act as your personal assistant, helping you understand complex ideas, generate code, and debug issues. These skills are crucial in almost every industry today, driving innovation and informed decision-making.

---

## 3. Essential Python Libraries & Tools

Here are some key tools you'll get familiar with. As you work through the module, practice using AI tools to get more information about them!

* [**NumPy:**](https://en.wikipedia.org/wiki/NumPy) The cornerstone for numerical computing in Python. It's essential for efficient array and matrix operations, forming the basis for many other data science libraries.
    * *Industry Use:* [High-performance calculations in scientific computing](https://en.wikipedia.org/wiki/High-performance_computing), [financial modeling](https://en.wikipedia.org/wiki/Financial_modeling), and [image processing](https://en.wikipedia.org/wiki/Digital_image_processing).
* [**Pandas:**](https://en.wikipedia.org/wiki/Pandas_(software)) Your primary tool for data manipulation and analysis. Pandas provides 'DataFrames,' which are flexible tables for cleaning, transforming, filtering, and exploring datasets.
    * *Industry Use:* [Data cleaning](https://en.wikipedia.org/wiki/Data_cleansing), [exploratory data analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis), [data preparation](https://en.wikipedia.org/wiki/Data_preparation) for machine learning across all sectors.
* **Matplotlib & Seaborn:** Python's leading visualization libraries. [Matplotlib](https://en.wikipedia.org/wiki/Matplotlib) offers extensive control for creating a wide array of static, animated, and interactive plots. [Seaborn](https://seaborn.pydata.org/), built on Matplotlib, provides a higher-level interface for drawing attractive statistical graphics.
    * *Industry Use:* Creating [dashboards](https://en.wikipedia.org/wiki/Dashboard_(computing)), generating reports, [visually exploring data trends and patterns](https://en.wikipedia.org/wiki/Data_and_information_visualization) for presentations and analysis.
* [**Scikit-learn (sklearn):**](https://en.wikipedia.org/wiki/Scikit-learn) A comprehensive and user-friendly library for machine learning. It includes tools for [classification](https://en.wikipedia.org/wiki/Statistical_classification), [regression](https://en.wikipedia.org/wiki/Regression_analysis), [clustering](https://en.wikipedia.org/wiki/Cluster_analysis), [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), [model selection](https://en.wikipedia.org/wiki/Data_modeling), and [data preprocessing](https://en.wikipedia.org/wiki/Data_preprocessing).
    * *Industry Use:* Building predictive models for [customer churn](https://en.wikipedia.org/wiki/Customer_attrition), [fraud detection](https://en.wikipedia.org/wiki/Data_analysis_for_fraud_detection), [medical diagnosis](https://en.wikipedia.org/wiki/Medical_diagnosis), and much more.
* [**Jupyter Notebooks/JupyterLab:**](https://en.wikipedia.org/wiki/Project_Jupyter) Interactive environments perfect for data science. They allow you to combine live code, equations, visualizations, and narrative text in a single document, making them ideal for exploration and sharing your work.
    * *Industry Use:* [Prototyping ML models](https://en.wikipedia.org/wiki/Software_prototyping), [data exploration](https://en.wikipedia.org/wiki/Data_exploration), creating [shareable analyses](https://en.wikipedia.org/wiki/Reproducibility) and tutorials.
* **Open-Source LLMs (e.g., via [Hugging Face Transformers](https://huggingface.co/docs/transformers/v4.36.1/en/index), [LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.com/)):**
    * [**Hugging Face Transformers:**](https://en.wikipedia.org/wiki/Hugging_Face) A library to access thousands of pre-trained models, including many powerful [open-source LLMs](https://en.wikipedia.org/wiki/Large_language_model). You can use these for tasks like [text generation](https://en.wikipedia.org/wiki/Natural_language_generation), [summarization](https://en.wikipedia.org/wiki/Automatic_summarization), or even [code generation](https://en.wikipedia.org/wiki/Vibe_coding) (depending on the model).
        * *How you can use it:* To understand code, generate boilerplate, or explore different ways to approach a problem.
    * [**LM Studio**](https://lmstudio.ai/) / [**Ollama:**](https://ollama.com/) Desktop applications that allow you to download and run various open-source LLMs locally on your computer. This offers a private and often cost-free way to interact with these AI models.
        * *How you can use it:* Ask conceptual questions ("Explain 'overfitting' in simple terms"), get help debugging Python errors by pasting the error message, or brainstorm project ideas.

---

## 4. Key Skills to Develop
Through the activities in this module, you will strengthen your ability to:
1.  **Define and Differentiate Core Concepts:** Clearly explain terms like [supervised](https://en.wikipedia.org/wiki/Supervised_learning)/[unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning), [classification](https://en.wikipedia.org/wiki/Statistical_classification)/[regression](https://en.wikipedia.org/wiki/Regression_analysis), and the data science lifecycle.
2.  **Master Basic Data Handling:** Independently use Pandas to load, inspect, clean, and transform diverse datasets.
3.  **Create Insightful Visualizations:** Generate and interpret various plots using Matplotlib and Seaborn to uncover data patterns.
4.  **Implement Foundational ML Models:** Apply basic [Scikit-learn algorithms](https://scikit-learn.org/stable/index.html) (e.g., [k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)) to solve simple classification tasks.
5.  **Leverage AI for Learning & Development:** Proactively use LLMs and AI tools to clarify doubts, generate code examples, debug issues, and explore data science problems more deeply.

---

## 5. Subtopics
This module is broken down into five manageable subtopics:
1.  **Understanding the Landscape: Data Science & Machine Learning Fundamentals**
2.  **Setting Up Your Python Data Science Environment**
3.  **Data Manipulation and Cleaning with Pandas**
4.  **Exploratory Data Visualization with Matplotlib & Seaborn**
5.  **Your First Machine Learning Model with Scikit-learn**

---

## 6. Experiential Use Cases
For each use case, actively engage with the problem. Don't hesitate to use an AI tool (like a local LLM via LM Studio/Ollama or an online one you have access to) if you get stuck or want to explore further!

### Subtopic 1: Understanding the Landscape: Data Science & Machine Learning Fundamentals

* **Use Case 1.1: Decoding Key Terminology**
    * **Problem Scenario:** You encounter terms like "AI," "Machine Learning," "Deep Learning," and "Data Science" frequently but are unsure of their precise meanings and relationships.
    * **Learning Outcome/Goal:** Create a personal glossary or a concept map clearly defining these terms and illustrating how they relate to one another.
    * **Python Tools & AI Resources:**
        * **Python:** N/A for direct coding.
        * **AI Resources:** Use an LLM (e.g., via LM Studio, Hugging Face, or other accessible platforms) with prompts like: "Explain the difference between AI and Machine Learning," "Where does Deep Learning fit into AI?", "Define Data Science and list its key components." Synthesize the LLM's explanations with your own understanding.

* **Use Case 1.2: Identifying ML in Your Daily Life**
    * **Problem Scenario:** Machine learning is all around us, but its applications aren't always obvious.
    * **Learning Outcome/Goal:** Identify and list at least five examples of machine learning you encounter in your daily life (e.g., recommendation systems, spam filters, voice assistants). For each, briefly describe what data might be used and what predictions or decisions are being made.
    * **Python Tools & AI Resources:**
        * **Python:** N/A.
        * **AI Resources:** Query an LLM: "What are common examples of machine learning in everyday applications?" or "How does Spotify use machine learning for music recommendations?". Use these as starting points for your own list and reflection.

* **Use Case 1.3: Outlining the Data Science Process**
    * **Problem Scenario:** You need a roadmap for tackling a data science project.
    * **Learning Outcome/Goal:** Outline the typical stages of the Cross-Industry Standard Process for Data Mining (CRISP-DM) or a similar data science lifecycle (e.g., business understanding, data understanding, data preparation, modeling, evaluation, deployment). For each stage, list one key activity and one potential challenge.
    * **Python Tools & AI Resources:**
        * **Python:** N/A.
        * **AI Resources:** Ask an LLM: "Describe the CRISP-DM framework for data mining" or "What are the typical steps in a data science project?". Try to get the LLM to explain potential pitfalls at each stage.

### Subtopic 2: Setting Up Your Python Data Science Environment

* **Use Case 2.1: Environment Configuration**
    * **Problem Scenario:** You need a robust and isolated Python environment for your data science projects.
    * **Learning Outcome/Goal:** Successfully install Anaconda or Miniconda, create a dedicated conda virtual environment, install the core libraries (NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, JupyterLab), and verify their installation by importing them in a Jupyter Notebook.
    * **Python Tools & AI Resources:**
        * **Python:** Use conda commands like `conda create -n my_ds_env python=3.9`, `conda activate my_ds_env`, `pip install numpy pandas matplotlib seaborn scikit-learn jupyterlab`. In Jupyter: `import numpy as np`, etc.
        * **AI Resources:** If you hit an error (e.g., "conda command not found" or a library installation fails), copy the exact error message and paste it into an LLM. Ask: "How do I fix this Python installation error: [paste error]?" or "Explain this conda error message."

* **Use Case 2.2: Jupyter Notebook Familiarization**
    * **Problem Scenario:** You are new to Jupyter Notebooks and need to learn how to use this interactive environment effectively.
    * **Learning Outcome/Goal:** Create a Jupyter Notebook where you practice: creating code cells and Markdown cells, executing Python code (e.g., simple calculations, variable assignments), using Markdown for titles and explanations, and saving your notebook.
    * **Python Tools & AI Resources:**
        * **Python:** Basic commands like `message = "Hello Jupyter!"`, `print(message)`, `x = 10; y = 20; print(x*y)`.
        * **AI Resources:** Ask an LLM: "What are common keyboard shortcuts for Jupyter Notebooks?" or "How can I create a bulleted list in a Jupyter Markdown cell?" or "Show me an example of a simple Jupyter Notebook structure."

* **Use Case 2.3: AI-Assisted Code Scaffolding**
    * **Problem Scenario:** You want to see how AI can help you get started with basic Python scripts.
    * **Learning Outcome/Goal:** Use an LLM to generate a Python script template that performs a common sequence of actions: (1) imports pandas, (2) defines a small sample DataFrame, (3) prints the DataFrame's dimensions, and (4) prints its first few rows. Then, run and modify this script in your Jupyter Notebook.
    * **Python Tools & AI Resources:**
        * **Python:** The script generated by the LLM.
        * **AI Resources:** Prompt an LLM: "Write a Python script using pandas that creates a small DataFrame with columns 'Name' and 'Age', then prints its shape and head." Critically review the generated code: Is it correct? Is it efficient?

### Subtopic 3: Data Manipulation and Cleaning with Pandas

*Note: For these use cases, you can find simple CSV datasets online (e.g., a basic student scores dataset, a small Titanic dataset subset, or create your own small CSV file). Alternatively, ask an LLM to generate a sample CSV content that you can save and use.*

* **Use Case 3.1: Loading and First Look at Data**
    * **Problem Scenario:** You have a CSV file and need to load it into Python and get a basic understanding of its content and structure.
    * **Learning Outcome/Goal:** Load a CSV dataset into a Pandas DataFrame. Use functions like `.head()`, `.tail()`, `.info()`, `.describe()`, and `.shape` to understand its dimensions, data types, summary statistics, and identify any immediate issues (like obviously wrong data types).
    * **Python Tools & AI Resources:**
        * **Python:** `import pandas as pd`, `df = pd.read_csv('your_dataset.csv')`, `print(df.head())`, `df.info()`, `print(df.describe())`.
        * **AI Resources:** Ask an LLM: "Explain the output of `df.info()` in pandas." or "What's the difference between `df.describe()` and `df.info()`?" or "How can I see the data types of each column in a pandas DataFrame?"

* **Use Case 3.2: Handling Missing Information**
    * **Problem Scenario:** Your dataset contains missing values (NaNs), which can cause problems for analysis and modeling.
    * **Learning Outcome/Goal:** Identify columns with missing values in your DataFrame. Implement at least two different strategies for handling them (e.g., fill numerical NaNs with the mean/median of the column, fill categorical NaNs with the mode, or drop rows/columns with excessive NaNs). Justify your choices.
    * **Python Tools & AI Resources:**
        * **Python:** `df.isnull().sum()`, `df['column_name'].fillna(value, inplace=True)`, `df.dropna(inplace=True)`.
        * **AI Resources:** "What are common strategies for handling missing data in pandas?" or "Generate Python code to fill missing values in a pandas column named 'Age' with its median." or "When is it appropriate to drop rows with missing values?"

* **Use Case 3.3: Subsetting and Filtering Data**
    * **Problem Scenario:** You need to isolate specific parts of your dataset for focused analysis.
    * **Learning Outcome/Goal:** Practice selecting specific columns from your DataFrame. Filter rows based on single and multiple conditions (e.g., select all entries where 'Age' > 30 and 'City' == 'London').
    * **Python Tools & AI Resources:**
        * **Python:** `df_subset = df[['column1', 'column3']]`, `filtered_df = df[df['Age'] > 30]`, `complex_filter = df[(df['Age'] > 30) & (df['City'] == 'London')]`.
        * **AI Resources:** "How do I select specific columns by name in a pandas DataFrame?" or "Show me how to filter pandas DataFrame rows based on multiple conditions." or "What's the difference between using `&` and `and` for pandas filtering?"

### Subtopic 4: Exploratory Data Visualization with Matplotlib & Seaborn

* **Use Case 4.1: Visualizing Single Variable Distributions**
    * **Problem Scenario:** You want to understand the distribution of individual numerical and categorical variables in your dataset.
    * **Learning Outcome/Goal:** For a numerical column, create and interpret a histogram and a box plot. For a categorical column, create and interpret a bar chart (count plot). Use Matplotlib or Seaborn.
    * **Python Tools & AI Resources:**
        * **Python:** `import matplotlib.pyplot as plt`, `import seaborn as sns`, `sns.histplot(df['numerical_col'], kde=True)`, `sns.boxplot(x=df['numerical_col'])`, `sns.countplot(x=df['categorical_col'])`.
        * **AI Resources:** "Generate Python code using Seaborn to create a histogram with a density curve for a pandas Series." or "How do I interpret a box plot?" or "Show me how to create a bar chart of value counts for a categorical column using Matplotlib."

* **Use Case 4.2: Exploring Relationships Between Two Variables**
    * **Problem Scenario:** You suspect there might be relationships between different variables in your dataset and want to visualize them.
    * **Learning Outcome/Goal:** Create a scatter plot to visualize the relationship between two numerical variables. Create a grouped bar chart or side-by-side box plots to compare a numerical variable across different categories of a categorical variable.
    * **Python Tools & AI Resources:**
        * **Python:** `sns.scatterplot(x='var1', y='var2', data=df)`, `sns.boxplot(x='categorical_var', y='numerical_var', data=df)`.
        * **AI Resources:** "Python code for a scatter plot with a regression line using Seaborn." or "How can I visualize the distribution of 'income' for different 'education levels' using box plots in Seaborn?"

* **Use Case 4.3: Enhancing Plots for Clarity with AI Assistance**
    * **Problem Scenario:** Your basic plots are informative, but you want to make them more professional and easier to understand by adding titles, labels, and customizing their appearance.
    * **Learning Outcome/Goal:** Take one of the plots you created earlier and improve it by adding a descriptive title, clear axis labels, and changing its color scheme or style. Try to get an LLM to suggest specific code modifications.
    * **Python Tools & AI Resources:**
        * **Python:** `plt.title('My Insightful Plot')`, `plt.xlabel('Independent Variable')`, `plt.ylabel('Dependent Variable')`, `sns.set_theme(style="whitegrid")`.
        * **AI Resources:** "How do I add a title and axis labels to a Seaborn plot?" or "Suggest ways to make my Matplotlib scatter plot more visually appealing." or "Generate Python code to change the color palette of a Seaborn plot."

### Subtopic 5: Your First Machine Learning Model with Scikit-learn

*Note: Use a simple, clean dataset for classification, like the Iris dataset (available in Scikit-learn: `from sklearn.datasets import load_iris`) or a simplified version of another common dataset.*

* **Use Case 5.1: Understanding the Task - Classification**
    * **Problem Scenario:** You want to predict a categorical outcome (e.g., type of flower, whether a customer will churn) based on some input features.
    * **Learning Outcome/Goal:** Load a suitable dataset. Clearly identify the features (independent variables, X) and the target variable (dependent variable, y) for a classification task. Explain why it's a classification problem.
    * **Python Tools & AI Resources:**
        * **Python:** `from sklearn.datasets import load_iris`, `iris = load_iris()`, `X = iris.data`, `y = iris.target`. (For custom data: `X = df[['feature1', 'feature2']]`, `y = df['target_class']`).
        * **AI Resources:** "Explain what features and target variables are in machine learning with an example." or "Using the Iris dataset, what are we trying to predict, and what information do we use to predict it?"

* **Use Case 5.2: Data Splitting and Model Training**
    * **Problem Scenario:** You need to train a machine learning model and then test its performance on unseen data.
    * **Learning Outcome/Goal:** Split your data (features X and target y) into training and testing sets. Train a simple classifier (e.g., k-Nearest Neighbors or Decision Tree) on the training data using Scikit-learn.
    * **Python Tools & AI Resources:**
        * **Python:** `from sklearn.model_selection import train_test_split`, `from sklearn.neighbors import KNeighborsClassifier` (or `DecisionTreeClassifier`), `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)`, `model = KNeighborsClassifier(n_neighbors=5)`, `model.fit(X_train, y_train)`.
        * **AI Resources:** "Generate scikit-learn code to split data into training and test sets with an 70/30 split." or "Explain the `random_state` parameter in `train_test_split`." or "How does a k-Nearest Neighbors classifier work? Explain simply." Ask an LLM to explain the `fit` method.

* **Use Case 5.3: Making Predictions and Evaluating Your Model**
    * **Problem Scenario:** Your model is trained, and now you need to see how well it performs.
    * **Learning Outcome/Goal:** Use your trained model to make predictions on the test set. Calculate and interpret the accuracy of your model. Reflect on what this accuracy score means.
    * **Python Tools & AI Resources:**
        * **Python:** `y_pred = model.predict(X_test)`, `from sklearn.metrics import accuracy_score`, `accuracy = accuracy_score(y_test, y_pred)`, `print(f"Model Accuracy: {accuracy:.2f}")`.
        * **AI Resources:** "How do I use a trained scikit-learn model to make predictions?" or "Explain what the accuracy score means in a classification context." or "What are other metrics besides accuracy for evaluating a classifier, and when would I use them?"

---

## 7. Self-Check Quiz
Test your understanding with these questions. Try to answer them first, then verify. You can use your notes, the provided materials, or even ask an LLM to help you understand the concepts if you're stuck (but try to figure it out yourself first!).

1.  Which of the following is the primary role of Pandas in Python data science?
    a)  Creating complex 3D visualizations.
    b)  Performing advanced statistical modeling.
    c)  Data manipulation, cleaning, and analysis, especially with tabular data.
    d)  Training deep learning neural networks.
    *(Answer: c)*

2.  Consider the Python code:
    ```python
    import numpy as np
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    c = a + b
    # What is c?
    ```
    What will be the value of `c`?
    a)  An error will occur.
    b)  `[1, 2, 3, 4, 5, 6]`
    c)  `[[1,4], [2,5], [3,6]]`
    d)  `[5, 7, 9]`
    *(Answer: d)*

3.  You have a dataset of customer information, and you want to predict if a customer will renew their subscription (Yes/No). This is an example of:
    a)  Regression
    b)  Clustering
    c)  Classification
    d)  Dimensionality Reduction
    *(Answer: c)*

4.  What is the main purpose of `train_test_split` in Scikit-learn?
    a)  To randomly shuffle the dataset.
    b)  To separate features from the target variable.
    c)  To divide the dataset into a training set (for model learning) and a test set (for unbiased evaluation).
    d)  To combine multiple datasets into one.
    *(Answer: c)*

5.  If you have a Pandas DataFrame `df` and want to see the first 7 rows, which command would you use?
    a)  `df.view(7)`
    b)  `df.head(7)`
    c)  `df.top(7)`
    d)  `df.show(7)`
    *(Answer: b)*

6.  You encounter a Python error: `NameError: name 'plt' is not defined`. You used `plt.plot(...)`. What is a likely cause you should investigate first?
    a)  You forgot to install the `plot` library.
    b)  You did not import Matplotlib's pyplot module, typically as `import matplotlib.pyplot as plt`.
    c)  Your data is not suitable for plotting.
    d)  `plt.plot` is not a valid function.
    *(Answer: b)*

7.  How can an open-source LLM (like one run via LM Studio) assist you if you're stuck on a Pandas operation?
    a)  It can directly access and modify your local Jupyter Notebook files.
    b)  It can provide explanations of Pandas functions, suggest code snippets for specific tasks, or help interpret error messages you provide.
    c)  It will automatically write and execute the entire data analysis project for you.
    d)  It can only provide general information about Python, not specific libraries like Pandas.
    *(Answer: b)*

8.  Which plot type is most suitable for visualizing the distribution of a single numerical variable and identifying potential outliers?
    a)  Scatter plot
    b)  Line plot
    c)  Bar chart
    d)  Box plot
    *(Answer: d)*

9.  What does `df.isnull().sum()` typically return for a Pandas DataFrame `df`?
    a)  The total number of missing values in the entire DataFrame.
    b)  A boolean DataFrame indicating True for missing values.
    c)  The sum of all values in the DataFrame, treating NaNs as zero.
    d)  A Pandas Series showing the count of missing values for each column.
    *(Answer: d)*

10. When training a k-Nearest Neighbors (k-NN) classifier using `KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)`, what does the `fit` method primarily do?
    a)  It makes predictions on the `X_train` data.
    b)  It evaluates the model's accuracy using `y_train`.
    c)  It essentially "stores" the `X_train` and `y_train` data so that during prediction, it can find the nearest neighbors.
    d)  It selects the optimal value for `k`.
    *(Answer: c)*

---

## 8. Further Reading & References

1.  **McKinney, W. (2022). *Python for Data Analysis* (3rd ed.). O'Reilly Media.**
    * The definitive guide to Pandas, written by its creator. Excellent for deep dives into data manipulation.
2.  **VanderPlas, J. (2016). *Python Data Science Handbook: Essential Tools for Working with Data*. O'Reilly Media.**
    * Also available free online ([https://jakevdp.github.io/PythonDataScienceHandbook/](https://jakevdp.github.io/PythonDataScienceHandbook/)). Covers NumPy, Pandas, Matplotlib, and Scikit-learn with clear examples.
3.  **Grus, J. (2019). *Data Science from Scratch: First Principles with Python* (2nd ed.). O'Reilly Media.**
    * Great for understanding the fundamentals by implementing them from scratch before relying on libraries.
4.  **Scikit-learn Official Documentation: User Guide** ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html))
    * Comprehensive, well-written documentation for all things Scikit-learn. Check for tutorials and examples.
5.  **Hugging Face Documentation (Transformers & Learn)** ([https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers) and [https://huggingface.co/learn/nlp-course](https://huggingface.co/learn/nlp-course) (for general NLP concepts which often use LLMs))
    * Essential for understanding how to find, use, and fine-tune open-source models, including LLMs that can assist your learning. Explore the models section too!
6.  **Pandas Official Documentation: Getting Started Tutorials** ([https://pandas.pydata.org/docs/getting_started/index.html](https://pandas.pydata.org/docs/getting_started/index.html))
    * Direct from the source, these tutorials are excellent for practical learning.

---
