{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the University of Arizona DataLab Github!","text":"<p>(URL: https://ua-datalab.github.io)</p>"},{"location":"#workshops","title":"Workshops","text":"<p>Our workshops focus on promoting Data Science Literacy among the university community. This ranges from satisfying curiosity to diverse topics in Statistics and Data Science to Machine Learning, to promote skill development in data modeling based on Machine Learning and Deep Learning algorithms across the wide science community of our University.</p>"},{"location":"#previous-datalab-workshops-learning-resources","title":"Previous DataLab Workshops Learning Resources","text":"<ul> <li>Advanced AI for Healthcare. Greg Chism.</li> <li>AI &amp; Data in Public Health: A Non-Coder's Toolkit!. Carlos Liz\u00e1rraga.</li> <li>Bioinformatics &amp; AI. Michele Cosi. </li> <li>Classical Machine Learning. Carlos Liz\u00e1rraga. </li> <li>Craking the Coding Interview. Mithun Paul.</li> <li>Data Engineering Technologies. Shashank Yadav.</li> <li>Data Science Tapas. </li> <li>Data Science Tools and Methods in Earth Sciences. Carlos Liz\u00e1rraga.</li> <li>Foundational Open Science Skills. Michele Cosi.  </li> <li>Deep Learning. Mithun Paul.</li> <li>Functional Open Science  Skills for AI/ML Applications. Michele Cosi, Carlos Liz\u00e1rraga, Enrique Noriega, Leonardo Soto Hernandez.</li> <li>Geospatial AI. Jeff Gillan.</li> <li>Graph Machine Learning. Shashank Yadav. </li> <li>Natural Language Processing for All.  Megh Krishnaswamy, Mithun Paul.</li> <li>Neural Networks. Brennon Huppenthal, Megh Krishnaswamy.</li> <li>NextGen Geospatial. Jeff Gillan.</li> <li>Practical AI for Research: LLMs, RAG &amp; Agentic Systems. Enrique Noriega.</li> <li>Prompt Engineering &amp; AI Application Deployment - GPT 101 (Previous version 2023). Tyson Swetnam. </li> <li>Previous Data Science Institute Workshops. Carlos Liz\u00e1rraga. </li> <li>Python &amp; AI for Data Analysis. Carlos Liz\u00e1rraga.</li> <li>Research Productivity. Rudy Salcido.</li> </ul>"},{"location":"#university-of-arizona-hpc","title":"University of Arizona HPC","text":"<ul> <li>HPC Workshops Calendar</li> <li>HPC Workshops and Training Materials</li> <li>HPC Documentation</li> </ul>"},{"location":"#other-learning-resources","title":"Other learning resources","text":"<ul> <li>Data Science Learning Resources wiki</li> <li>DataLab Projects</li> <li>AI Tools Landscape</li> <li>AI Landscape &amp; Prompt Engineering</li> </ul>"},{"location":"#university-of-arizona-datalab-social","title":"University of Arizona DataLab Social","text":"<ul> <li> Github</li> <li> Linkedin</li> <li> Bluesky Social</li> <li> Facebook Page</li> <li> Workshop videorecordings YouTube Channel </li> </ul> <p> Questions / Contact: Please join the  UA Data Science Slack channel: <pre><code>#datalab-chatter\n</code></pre></p> <p> Please visit the University of Arizona Data Science Institute Events Calendar for more detailed information.</p> <p>University of Arizona DataLab, is a strategic program between the University of Arizona's Data Science Institute, and CyVerse.</p> <p> </p>"},{"location":"DSecosystem/","title":"Data Science Ecosystem","text":""},{"location":"DSecosystem/#the-data-science-ecosystem","title":"The Data Science Ecosystem","text":"<p>The University of Arizona campus has diverse data science needs served by specialized campus units. These units collaborate both formally and informally, each providing unique expertise to specific audiences. Together, they deliver comprehensive campus support that would be impossible for any single unit alone.</p>"},{"location":"DSecosystem/#data-science-organizational-structure","title":"Data Science organizational structure","text":"<ul> <li> <p>Data Science Team. Delivers workshops on reproducible research for CALES/ALVSCE researchers, focusing on ecology and bioinformatics. Offers in-depth support through the Data Science Incubator for research analysis and stakeholder outreach.</p> </li> <li> <p>DataLab. Is a joint initiative of the Data Science Institute and the Institute for Computation and Data-enabled Insight offering intermediate workshops on AI, machine learning, genomics, and natural language processing. </p> </li> <li> <p>Research Bazaar. A research computing community hosting twice-weekly office hours for advanced users on and off campus.</p> </li> <li> <p>Research Computing. Provides HPC resources support through workshops and consultations. Based in UITS, serves users with computational expertise or those needing national computing resources.</p> </li> <li> <p>StatLab BIO5. StatLab offers campus-wide statistical consulting through free initial consultations, paid in-depth support, and grant collaboration options.</p> </li> <li> <p>University Libraries. Provide introductory workshops and consultations focused on scientific programming, geospatial analysis, and data management for reproducible research. </p> </li> </ul>"},{"location":"about/","title":"About","text":"<p>U of A IT Summit 2024 Presentation. Learning &amp; Maker Space for ML/AI: University of Arizona Data Lab</p>"},{"location":"about/#mission","title":"Mission","text":"<p>The University of Arizona DataLab's mission is to assist our community in developing data science skills. This is achieved through various training sessions, workshops, and consultations, which aid research groups in incorporating software tools and AI-based systems. These resources enhance the cognitive skills necessary for their knowledge-discovery processes.</p>"},{"location":"about/#vision","title":"Vision","text":"<p>The University of Arizona DataLab serves as a key hub for innovation. It fosters the assimilation and adoption of new AI-related technologies. This ensures the competitiveness of the University of Arizona academic community, which is at the forefront of knowledge discovery.</p>"},{"location":"about/#description","title":"Description","text":"<p>The landscape of machine learning (ML) and artificial intelligence (AI) is continually evolving, with rapid advancements constantly emerging. For successful and measured adoption of ML/AI in any field, learning and experimenting with these advancements is crucial. This is important for University of Arizona researchers to secure funding and train an ML/AI fluent workforce.</p> <p>The University of Arizona DataLab serves as a hub for the University of Arizona community, providing extracurricular and non-formal training in ML/AI tools and technologies not covered in current academic programs. It offers short-format workshops across various knowledge domains. Its open learning environment is ideal for studying ML/AI, where all staff members are welcome to participate.</p> <p>The University of Arizona DataLab strives to guide learners from curiosity to competence in AI. It complements learning activities with consultations on research projects, assisting research labs to incorporate ML/AI methods into their workflow. It also provides support in onboarding and training students and staff involved in implementing these solutions. Additionally, it facilitates codefest/hackathons on cross-cutting topics of interest.</p> <p>The University of Arizona DataLab collaborates with multiple university entities, including the UITS Research Computing group and Data Science training efforts through other colleges and units. It aims to foster a vibrant ecosystem of learning and exploration in the ML/AI space for the University of Arizona community.</p> <p>This talk will outline the progress and roadmap for non-formal learners on how to utilize DataLab. It will also provide guidance for departments and other units on how to participate in DataLab to showcase their activities and highlight the range of ML/AI learning opportunities available across our campus.</p>"},{"location":"about/#partnerships","title":"Partnerships","text":"<p>The University of Arizona DataLab  is an effective organizational structure of a collaboration between the University of Arizona's: Data Science Institute, CyVerse, and the Institute for Computation &amp; Data-Enabled Insight for fostering and accelerating research in Applied AI/ML by providing access to expertise, data, collaboration, advanced infrastructure, and training opportunities.</p>"},{"location":"aiverde/","title":"Ai-VERDE","text":"<p> <p> <p> AI-VERDE is a custom-built AI integration platform that     provides research teams and educators with seamless, secure,     and compliant access to essential building blocks for exploring     and developing GenAI powered solutions.     VERDE provides connectivity to a wide range of commercial     LLM providers (GPT-4, Gemini, Claude etc.) or to self-hosted     on-premise models (LLAMA4, PHI4, Gemma3 etc), and     inference service providers from NSF supported infrastructure    (Jetstream2, NRP, AnvilGPT etc.)        VERDE integrates access from multiple LLM providers into a     unified managed platform, providing streamlined access to     custom agent workflows and fine-tuned models, chat interfaces,     RAG pipelines, with the freedom to connect to VERDE using any     web or desktop application, integrated code assistants that     support OpenAI API.        VERDE facilitates research agility and cost control by providing     instructors and team leads with granular budget and access     control, prompt libraries, and defined duration based access for     workshop and course based enrollment.       <p> <p> <p>AI VERDE Login: https://chat.cyverse.ai/ <p>Read Paper: AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based Resources For Educational Institutions. DOI:  https://doi.org/10.48550/arXiv.2502.09651.  <p>Read more about Ai-VERDE <p> </p>"},{"location":"apps-dev/","title":"AI/ML Development","text":""},{"location":"apps-dev/#aiml-applications-development-and-implementation","title":"AI/ML Applications Development and Implementation","text":"<p>When research groups need customized solutions for knowledge discovery, grant proposal writing enhancement, improving learning outcomes in academic programs, or addressing other identified needs within the university's core functions, our DataLab team can find AI-based solutions to meet these requirements.</p> <p>The DataLab team contributes to the following developments:</p> <ul> <li>Offering AI-based chatbot systems that act as personal tutors for students (Chatur/WILMA project)</li> <li>Creating AI-based solutions for information discovery in new scientific publications</li> <li>Implementing an AI-based system to assist in antenna design</li> <li>Using a Large Language Model for Information Extraction</li> <li>Other</li> </ul>"},{"location":"capstone/","title":"Capstone Projects","text":""},{"location":"capstone/#u-of-a-capstone-projects","title":"U of A Capstone Projects","text":"<p>(Image credit: Jo Szczepanska. Unsplash.com)</p> <p>The U of A DataLab develops projects that enable students to apply their AI/Machine Learning knowledge and skills to real-world problems, demonstrating what they've learned throughout their academic program.</p>"},{"location":"capstone/#current-ongoing-projects","title":"Current ongoing projects","text":"<ul> <li> <p>PubMed Central RAG Agent Chatbot Agent-based Retrieval Augmented Generation of biological sciences research using LLMs. The outcome of this project will be a conversational chatbot able to retrieve and respond questions from an expansive database created with hundeds of thousands of papers.</p> </li> <li> <p>Deep Learning Pose Estimation. Tracking animal behavior noninvasively is crucial for many scientific fields. Extracting animal poses without markers is essential in biomechanics, genetics, ethology, and neuroscience but challenging in dynamic backgrounds.</p> </li> <li> <p>Improving Navigation and Accessibility for the Visually Impaired. Visually impaired individuals at the University of Arizona face significant navigation challenges across campus buildings, streets, and walkways. This project explores solutions to enhance accessibility through detailed spatial information and precise guidance for navigation around the University of Arizona campus and surrounding areas. Recent advances in Large Language Models and Vision-Language Models offer promising new opportunities for improved assistive navigation.</p> </li> </ul>"},{"location":"capstone/#capstone-products-repository","title":"Capstone Products Repository","text":"<ul> <li>Products</li> </ul> <p>Created: 02/24/2025 Carlos Liz\u00e1rraga.</p> <p>Updated: 05/09/2025 Carlos Liz\u00e1rraga.</p> <p>DataLab, Data Science Institute, University of Arizona. </p> <p> CC BY-NC-SA 4.0</p> <p></p>"},{"location":"consultations/","title":"Consultations","text":""},{"location":"consultations/#consultations","title":"Consultations","text":"<p>When a research group needs detailed support in specific algorithms for data modeling, the DataLab team and associate members can offer consultation services. These services ensure the success of data-driven research across various knowledge domains. Consultations include support in specialized software tools and specific algorithms, and providing access to computational cloud infrastructure like CyVerse, Jetstream2, and HPC. Furthermore, graduate students are guided on best practices in source code development, data management, and modeling practices. Assistance is also provided in the available scientific computing environments to support specific domain research areas.</p>"},{"location":"consultations/#consultation-services-at-the-university-of-arizona-datalab","title":"Consultation services at the University of Arizona DataLab","text":"<ul> <li>AI/ML applications research software</li> <li>Cloud based analytic tools</li> <li>Data mining &amp; analytics tools</li> <li>Data protection &amp; validation</li> <li>Data visualization tools</li> <li>Open Science &amp; Reproducible Research </li> </ul> <p>AI Applied Research Topics:</p> <p>Machine Learning &amp; Deep Learning</p> <ul> <li>Convolutional Neural Networks for Text Classification</li> <li>Generative AI for Vision: Diffusion Models and GANs</li> <li>Large Language Models (LLM)</li> <li>Large Language Models Personalization</li> <li>Natural Language Processing</li> <li>Object Detection and Image Segmentation</li> <li>Prompt Engineering</li> <li>Speech Processing</li> <li>Vision Language Models</li> <li>Vision Transformers - VIT</li> </ul> <p>Privacy-Preserving</p> <ul> <li>Differential Privacy</li> <li>Federated Learning</li> <li>Model Compression</li> <li>Personally Identifiable Information (PII)</li> </ul> <p>To schedule an initial DataLab consultation, please contact: Carlos Liz\u00e1rraga (clizarraga@arizona.edu).</p>"},{"location":"events/","title":"Events Calendar","text":"<p>(URL: https://ua-datalab.github.io/events/) </p>"},{"location":"events/#your-datalab-friendly-weekly-remainder","title":"Your DataLab friendly weekly remainder!","text":"<p>Join us for an exciting series of hands-on workshops and learning opportunities!                    </p> <p>Find all U of A DataLab activities  on the Data Science Institute calendar events.</p>"},{"location":"events/#regular-weekly-data-science-related-activities-at-the-university-of-arizona","title":"Regular weekly data science related activities at the University of Arizona","text":""},{"location":"events/#tuesdays","title":"Tuesdays","text":"<p>9:00 AM - 11:00 AM. Data &amp; Viz Drop-in. Your one-stop-shop for R, Python, Data Management and Data Visualization help.      In-person @ Main Library B201, Data Studio - CATalyst Studios  (Weekly event)</p> <p>3:30 PM - 4:30 PM. AI Makerspace MeetUp. [Register].    In-person @ Snakes &amp; Lattes - Tucson (988 E University Blvd, Tucson, AZ).   (Starts 08/26;  Ends 10/28)</p>"},{"location":"events/#wednesdays","title":"Wednesdays","text":"<p>8:30 AM - 10:30 AM. Coffee &amp;  Code.     In-person @ Catalyst Caf\u00e9 - BSRL Lobby  (Weekly event)</p> <p>2:00 PM - 6:00 PM. Code Commons   In-person @ CATalyst Studios. Main Library  (Weekly event)</p>"},{"location":"events/#thursdays","title":"Thursdays","text":"<p>4:00 PM - 7:00 PM. Hacky Hour.    In-person @ Snakes &amp; Lattes - Tucson (988 E University Blvd, Tucson, AZ).   (Weekly event)  </p>"},{"location":"events/#fridays","title":"Fridays","text":"<p>10:00 AM - 11:00 AM. CyVerse Office Hours  [Register]    In-person @ Catalyst Caf\u00e9 - BSRL Lobby </p> <p>Questions / Contact: Please join the UA Data Science Slack channel: <pre><code>#datalab-chatter\n</code></pre></p> <p>Additional learning resources:  U of A DataLab GitHub repository.</p> <p>Watch previous U of A DataLab workshops on our UArizona DataLab YouTube Channel.</p> <p>We look forward to seeing you at the workshops. Register now!</p> <p>Please feel free to share this with your friends and colleagues.</p> <p>Updated 04/23/2025 (C. Liz\u00e1rraga - clizarraga AT arizona.edu). UArizona DataLab.</p>"},{"location":"members/","title":"DataLab Team","text":""},{"location":"members/#university-of-arizona-datalab-team","title":"University of Arizona DataLab Team","text":""},{"location":"members/#staff","title":"Staff","text":"<ul> <li>Michele Cosi. Data Science Institute / CyVerse.</li> <li> <p>Jeff Gillan. Data Science Institute /  CyVerse.</p> </li> <li> <p>Carlos Liz\u00e1rraga. Data Science Institute.</p> </li> <li>Enrique Noriega. Data Science Institute.</li> </ul>"},{"location":"members/#associate-members","title":"Associate Members","text":"<ul> <li>Devin Bayly. Research Technologies. </li> <li>Andrew Bennett. Hydrology and Atmospheric Sciences.</li> <li>Chi-Kwan Chan. Astronomy and Stewards Observatory.</li> <li>Greg Chism. College of Information.</li> <li>Nick Eddy. Data Science Institute. </li> <li>Tina L. Johnson. Data Science Institute.</li> <li>Eung-Joo Lee. Electrical &amp; Computer Engineering. </li> <li>Nirav Merchant. Data Science Institute.</li> <li>Onicio B. Leal-Neto. Mel &amp; Enid Zuckerman College of Public Health.</li> <li>Maliaca Oxnam. Data Science Institute.</li> <li>Rudy G. Salcido. Data Science Institute.</li> <li>Tyson Swetnam. Data Science Institute / CyVerse.</li> <li>Francesca Vitali. Neurology; Center for Innovation in Brain Sciences (CIBS), Bio5 Institute.</li> <li>Chicheng Zhang. Computer Science.</li> </ul>"},{"location":"members/#students-members","title":"Students Members","text":"<ul> <li>Eli Kondwani Mark</li> <li>Mandira Bhowmik</li> <li>Megh Krishnaswamy. Linguistics.</li> <li>Zi Kang Deng</li> </ul>"},{"location":"members/#former-students-members","title":"Former Students Members","text":"<ul> <li>Ajay Perumbeti. College of Medicine, PHX.</li> <li>Ankit Pal. College of Information.</li> <li>Austin Medina. College of Engineering.</li> <li>Brenda Huppenthal. Computer Science.</li> <li>Linda Engelman. Chemical and Environmental </li> <li>Patrick Lohr. Computational Materials Sciences. Engineering.</li> <li>Shashank Yadav. Biomedical Engineering. </li> </ul>"},{"location":"mlpaths/","title":"Mlpaths","text":""},{"location":"mlpaths/#data-science-learning-path","title":"Data Science Learning Path","text":"<p>We present 12 topics in the data science learning path, providing learning objectives, related skills, subtopics, and references/resources for each. The goal is to give graduate students a structured and comprehensive program to acquire data science expertise, including hands-on experience with real-world open-source tools and libraries.</p> <pre><code>timeline\n    title Machine Learning Learning Path\n    A. General Data Science : 1. Introduction to Data Science and Machine Learning\n                         : 2. Python for Data Science\n                 : 3. Ethical Considerations in Data Science\n    B. Statistics : 4. Statistical Learning and Regression Models\n    C. Classical Machine Learning : 5. Classification Algorithms\n                 : 6. Ensemble Methods\n                 : 7. Unsupervised Learning\n    D. Deep Learning : 8. Introduction to Deep Learning\n                : 9. Recurrent Neural Networks and Sequence Models\n            : 10. Generative Models\n            : 11. Transfer Learning and Fine-tuning\n    E. Continuous Integration / Continuous Deployment  : 12. Model Deployment and Productionization\n</code></pre>"},{"location":"mlpaths/#a-general-data-science","title":"A: General Data Science","text":""},{"location":"mlpaths/#1-introduction-to-data-science-and-machine-learning","title":"1. Introduction to Data Science and Machine Learning","text":"Topic description <p>Learning Objective: Understand the fundamental concepts of data science and machine learning, and their real-world applications.</p> <p>Related Skills:</p> <ul> <li>Defining and framing data science problems</li> <li>Identifying appropriate machine learning techniques for different tasks</li> <li>Distinguishing between supervised and unsupervised learning</li> </ul> <p>Subtopics:</p> <ul> <li>Definition and scope of data science: Lies, Damned Lies, and Data    Science. B\u00e9atrice Moissinac.</li> <li>Overview of machine learning algorithms (regression, classification, clustering): Introduction to  Machine Learning. Developers Google.</li> <li>Applications of data science in various industries (e.g., healthcare, finance, marketing): Data Science Applications Across 10 Different Industries. Rice University.</li> <li>Ethical considerations in data science: A Guide for Ethical Data Science. Royal Statistical Society.</li> <li>Hands-on introduction to machine learning using Python and scikit-learn: Machine Learning Crash Course. Google Developers. </li> </ul> <p>References and Resources:</p> <ul> <li>Data Science; Concepts and Practice. V. Kotu and B. Deshpande. </li> <li>Data Science for Beginners - A curriculum. Microsoft 10-week, 20-lesson curriculum all about Data Science. </li> <li>General Data Science Learning Resources. Data Science Institute, University of Arizona.</li> </ul>"},{"location":"mlpaths/#2-python-for-data-science","title":"2. Python for Data Science","text":"Topic description <p>Learning Objective: Develop proficiency in using Python for data manipulation, analysis, and visualization.</p> <p>Related Skills:</p> <ul> <li>Mastering Python syntax and data structures</li> <li>Utilizing NumPy for efficient numerical operations</li> <li>Applying Pandas for data ingestion, cleaning, and transformation</li> </ul> <p>Subtopics:</p> <ul> <li>Python programming basics (variables, data types, control structures, functions): Chap 2., and Chap 3, McKinney.</li> <li>NumPy arrays and universal functions: Chap 4. McKinney </li> <li>Pandas DataFrames and Series for data manipulation: Chap 5., Chap 6., and Chap 7., McKinney </li> <li>Data visualization with Matplotlib and Seaborn: Matplotlib tutorials, and Seaborn tutorials.</li> <li>Integrating Python with data science libraries (scikit-learn, TensorFlow, PyTorch)</li> </ul> <p>References and Resources:</p> <ul> <li>Python for Data Analysis, 3E. Wes McKinney.</li> <li>Python Data Science Handbook.  Jake VanderPlas.</li> <li>Data Visualization: A practical introduction. Kieran Healy.</li> <li>Fundamentals of Data Visualization. Claus O. Wilke.</li> <li>Python Programming Language Learning Resources. Data Science Institute, University of Arizona.</li> </ul>"},{"location":"mlpaths/#3-ethical-considerations-in-data-science","title":"3. Ethical Considerations in Data Science","text":"Topic description <p>Learning Objective: Develop an understanding of the ethical implications and responsible practices in data science.</p> <p>Related Skills:</p> <ul> <li>Identifying and mitigating bias in data and models</li> <li>Ensuring fair and equitable decision-making</li> <li>Protecting privacy and data security</li> </ul> <p>Subtopics:</p> <ul> <li>Bias and fairness in machine learning</li> <li>Interpretability and explainability of models</li> <li>Privacy-preserving techniques (differential privacy, federated learning)</li> <li>Data provenance and provenance tracking</li> <li>Responsible AI principles and guidelines</li> </ul> <p>References and Resources:</p> <ul> <li>\"Ethical Algorithms\" by Michael Kearns and Aaron Roth</li> <li>\"Artificial Intelligence: A Modern Approach\" by Stuart Russell and Peter Norvig</li> <li>Coursera course \"AI Ethics\" by DeepLearning.AI</li> </ul>"},{"location":"mlpaths/#b-statistics","title":"B: Statistics","text":""},{"location":"mlpaths/#4-statistical-learning-and-regression-models","title":"4. Statistical Learning and Regression Models","text":"Topic description <p>Learning Objective: Understand and apply statistical learning techniques, with a focus on regression models.</p> <p>Related Skills:</p> <ul> <li>Fitting and evaluating linear regression models</li> <li>Applying logistic regression for classification tasks</li> <li>Interpreting model coefficients and making predictions</li> </ul> <p>Subtopics:</p> <ul> <li>Simple and multiple linear regression</li> <li>Assumptions and diagnostics of linear regression</li> <li>Logistic regression for binary classification</li> <li>Evaluating model performance (R-squared, accuracy, precision, recall, F1-score)</li> <li>Regularization techniques (Ridge, Lasso, Elastic Net)</li> </ul> <p>References and Resources:</p> <ul> <li>\"An Introduction to Statistical Learning\" by Gareth James et al.</li> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li> <li>Coursera course \"Machine Learning\" by Andrew Ng</li> </ul>"},{"location":"mlpaths/#c-classical-machine-learning","title":"C: Classical Machine Learning","text":""},{"location":"mlpaths/#5-classification-algorithms","title":"5. Classification Algorithms","text":"Topic description <p>Learning Objective: Acquire knowledge of various classification algorithms and their application in real-world problems.</p> <p>Related Skills:</p> <ul> <li>Implementing and evaluating decision tree classifiers</li> <li>Applying k-nearest neighbors for classification</li> <li>Understanding the principles of support vector machines</li> </ul> <p>Subtopics:</p> <ul> <li>Decision tree classification</li> <li>K-nearest neighbors (KNN) algorithm</li> <li>Support vector machines (SVMs)</li> <li>Evaluating classification models (accuracy, precision, recall, F1-score, ROC-AUC)</li> <li>Handling class imbalance (oversampling, undersampling, SMOTE)</li> </ul> <p>References and Resources:</p> <ul> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li> <li>\"Hands-On Machine Learning with Scikit-Learn and TensorFlow\" by Aur\u00e9lien G\u00e9ron</li> <li>Udacity course \"Intro to Machine Learning\"</li> </ul>"},{"location":"mlpaths/#6-ensemble-methods","title":"6. Ensemble Methods","text":"Topic description <p>Learning Objective: Explore ensemble techniques for improving the performance of machine learning models.</p> <p>Related Skills:</p> <ul> <li>Implementing random forest algorithms</li> <li>Understanding the principles of gradient boosting</li> <li>Applying bagging and boosting techniques to enhance model accuracy</li> </ul> <p>Subtopics:</p> <ul> <li>Random forest classification and regression</li> <li>Gradient boosting with XGBoost and LightGBM</li> <li>Bagging and boosting (AdaBoost, Gradient Boosting)</li> <li>Hyperparameter tuning for ensemble methods</li> <li>Feature importance and interpretation in ensemble models</li> </ul> <p>References and Resources:</p> <ul> <li>\"Hands-On Machine Learning with Scikit-Learn and TensorFlow\" by Aur\u00e9lien G\u00e9ron</li> <li>\"Introduction to Statistical Learning\" by Gareth James et al.</li> <li>Kaggle micro-course on \"Advanced Ensembling\"</li> </ul>"},{"location":"mlpaths/#7-unsupervised-learning","title":"7. Unsupervised Learning","text":"Topic description <p>Learning Objective: Gain proficiency in unsupervised learning techniques for data exploration and pattern discovery.</p> <p>Related Skills:</p> <ul> <li>Implementing K-means clustering algorithms</li> <li>Applying principal component analysis (PCA) for dimensionality reduction</li> <li>Identifying anomalies and outliers in data</li> </ul> <p>Subtopics:</p> <ul> <li>K-means clustering</li> <li>Hierarchical clustering</li> <li>Principal component analysis (PCA)</li> <li>Anomaly detection techniques (Isolation Forest, One-Class SVM)</li> <li>Dimensionality reduction methods (t-SNE, UMAP)</li> </ul> <p>References and Resources:</p> <ul> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li> <li>\"Hands-On Unsupervised Learning Using Python\" by Ankur Patel</li> <li>Coursera course \"Cluster Analysis in Data Mining\" by University of Illinois</li> </ul>"},{"location":"mlpaths/#d-deep-learning","title":"D: Deep Learning","text":""},{"location":"mlpaths/#8-introduction-to-deep-learning","title":"8. Introduction to Deep Learning","text":"Topic description <p>Learning Objective: Develop an understanding of the fundamental concepts and architectures of deep neural networks.</p> <p>Related Skills:</p> <ul> <li>Constructing and training feedforward neural networks</li> <li>Applying convolutional neural networks for image-related tasks</li> <li>Selecting appropriate activation functions and optimization techniques</li> </ul> <p>Subtopics:</p> <ul> <li>Artificial neural networks (ANNs) and their structure</li> <li>Activation functions (sigmoid, ReLU, tanh)</li> <li>Feedforward neural networks and their training</li> <li>Convolutional neural networks (CNNs) for image recognition</li> <li>Hyperparameter tuning and optimization techniques</li> </ul> <p>References and Resources:</p> <ul> <li>\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li> <li>\"Deep Learning with Python\" by Fran\u00e7ois Chollet</li> <li>Coursera course \"Deep Learning Specialization\" by deeplearning.ai</li> </ul>"},{"location":"mlpaths/#9-recurrent-neural-networks-and-sequence-models","title":"9. Recurrent Neural Networks and Sequence Models","text":"Topic description <p>Learning Objective: Understand the principles of recurrent neural networks and their applications in sequence-to-sequence problems.</p> <p>Related Skills:</p> <ul> <li>Implementing LSTM and GRU models for sequence modeling</li> <li>Applying recurrent neural networks for time series forecasting</li> <li>Generating text and other sequential data using RNNs</li> </ul> <p>Subtopics:</p> <ul> <li>Recurrent neural networks (RNNs)</li> <li>Long short-term memory (LSTMs)</li> <li>Gated recurrent units (GRUs)</li> <li>Sequence-to-sequence modeling</li> <li>Time series forecasting with RNNs</li> </ul> <p>References and Resources:</p> <ul> <li>\"Deep Learning for Time Series Forecasting\" by Jason Brownlee</li> <li>\"Natural Language Processing with Python\" by Steven Bird et al.</li> <li>Coursera course \"Sequence Models\" by deeplearning.ai</li> </ul>"},{"location":"mlpaths/#10-generative-models","title":"10. Generative Models","text":"Topic description <p>Learning Objective: Explore generative models and their applications in synthesizing new data.</p> <p>Related Skills:</p> <ul> <li>Implementing generative adversarial networks (GANs)</li> <li>Applying variational autoencoders (VAEs) for image and text generation</li> <li>Evaluating the performance of generative models</li> </ul> <p>Subtopics:</p> <ul> <li>Generative adversarial networks (GANs)</li> <li>Variational autoencoders (VAEs)</li> <li>Generative modeling for images, text, and other data types</li> <li>Evaluating generative models (Inception Score, FID, BLEU)</li> <li>Applications of generative models (data augmentation, creative generation)</li> </ul> <p>References and Resources:</p> <ul> <li>\"Generative Adversarial Networks\" by Ian Goodfellow et al.</li> <li>\"Variational Autoencoders\" by Diederik Kingma and Max Welling</li> <li>Deeplearning.ai course \"Generative Adversarial Networks (GANs)\"</li> </ul>"},{"location":"mlpaths/#11-transfer-learning-and-fine-tuning","title":"11. Transfer Learning and Fine-tuning","text":"Topic description <p>Learning Objective: Understand the principles of transfer learning and how to leverage pre-trained models for various tasks.</p> <p>Related Skills:</p> <ul> <li>Applying feature extraction with pre-trained models</li> <li>Finetuning pre-trained models for domain-specific tasks</li> <li>Evaluating the performance of transfer learning approaches</li> </ul> <p>Subtopics:</p> <ul> <li>Concept of transfer learning</li> <li>Feature extraction using pre-trained models (e.g., VGG, ResNet, BERT)</li> <li>Finetuning pre-trained models for specific applications</li> <li>Domain adaptation and dataset shift</li> <li>Evaluating transfer learning performance</li> </ul> <p>References and Resources:</p> <ul> <li>\"Transfer Learning with Deep Learning\" by Sebastian Ruder</li> <li>\"Practical Deep Learning for Cloud, Mobile, and Edge\" by Anirudh Koul et al.</li> <li>Coursera course \"Convolutional Neural Networks\" by deeplearning.ai</li> </ul>"},{"location":"mlpaths/#e-continuous-integration-continuous-deployment","title":"E: Continuous Integration / Continuous Deployment","text":""},{"location":"mlpaths/#12-model-deployment-and-productionization","title":"12. Model Deployment and Productionization","text":"Topic description <p>Learning Objective: Gain knowledge on how to deploy and maintain machine learning models in production environments.</p> <p>Related Skills:</p> <ul> <li>Containerizing models using Docker</li> <li>Deploying models on cloud platforms (e.g., AWS, GCP, Azure)</li> <li>Monitoring and maintaining production models</li> </ul> <p>Subtopics:</p> <ul> <li>Containerization with Docker</li> <li>Cloud deployment on AWS, GCP, and Azure</li> <li>Serving models with Flask, FastAPI, or Streamlit</li> <li>Model monitoring and logging</li> <li>Continuous integration and deployment (CI/CD) pipelines</li> </ul> <p>References and Resources:</p> <ul> <li>\"Deploying Machine Learning Models\" by Abhishek Thakur</li> <li>\"Kubernetes in Action\" by Marko Luk\u0161a</li> <li>Coursera course \"Machine Learning Engineering for Production (MLOps)\" by deeplearning.ai</li> </ul>"},{"location":"mlpaths/#working-with-different-data-types","title":"Working with different data types.","text":"<p>Next you will find five specialized data science learning paths that branch off from the core topics in the previous section. Each specialized path includes a learning objective, related skills, subtopics, and references/resources.</p> <pre><code>  flowchart LR;\n      A@{shape:processes, label: \"Data types\"}--&gt;B[\"`**Working with Numeric and Categorical Data**`\"];\n      A--&gt;C[\"`**Computer Vision and Image-based Learning**`\"];\n      A--&gt;D[\"`**Time Series Analysis and Forecasting**`\"];\n      A--&gt;E[\"`**Natural Language Processing**`\"];\n      A--&gt;F[\"`**Speech and Audio Processing**`\"];\n      click B href \"https://github.com/ua-datalab/mlpaths/wiki/Working-with-Numeric-and-Categorical-Data\" \"Open this in a new tab\" _blank\n      click C href \"https://github.com/ua-datalab/mlpaths/wiki/Computer-Vision-and-Image%E2%80%90based-Learning\" \"Open this in a new tab\" _blank\n      click D href \"https://github.com/ua-datalab/mlpaths/wiki/Time-Series-Analysis-and-Forecasting\" \"Open this in a new tab\" _blank\n      click E href \"https://github.com/ua-datalab/mlpaths/wiki/Natural-Language-Processing\" \"Open this in a new tab\" _blank\n      click F href \"https://github.com/ua-datalab/mlpaths/wiki/Computer-Vision-and-Image%E2%80%90based-Learning\" \"Open this in a new tab\" _blank\n\n</code></pre>"},{"location":"mlpaths_grids/","title":"Take the LEAD with Datalab.","text":""},{"location":"mlpaths_grids/#the-datalab-lead-program-learn-experience-advance-develop","title":"The Datalab LEAD Program: Learn, Experience, Advance, Develop.","text":""},{"location":"mlpaths_grids/#machine-learningartificial-intelligence-learning-paths","title":"Machine Learning/Artificial Intelligence Learning Paths","text":"<p>We present 20 topics in the data science learning path, providing learning objectives, related skills, subtopics, and references/resources for each. The goal is to give graduate students a structured and comprehensive program to acquire data science expertise, including hands-on experience with real-world open-source tools and libraries.</p> <pre><code>timeline\n     A. General Data Science  : A1. Introduction to Data Science and Machine Learning\n                         : A2. Data Analyis with Pandas\n                 : A3. Data Visualization\n                 : A3. Ethical Considerations in Data Science\n     B. Statistics : B1. Descriptive Statistics\n                  : B2. Probability Distributions\n          : B3. Inferential Statistics\n          : B4. Bayesian Statistics\n     C. Machine Learning  : C1. Machine Learning with Scikit-Learn\n                    : C2. Supervised Learning\n                 : C3. Unsupervised Learning\n             : C4. Ensemble Methods\n     D. Deep Learning  : D1. Neural Networks with PyTorch\n            : D2. Transformers with HuggingFace\n            : D3. GenAI 1 &lt;br&gt; (LLM, RAG) \n            : D4. GenAI 2 &lt;br&gt; (Multimodal LLMs)\n            : D5. Agents and MCP\n     E. Continuous Integration / Continuous Deployment : E1. MLOps\n                                       : E2. LLMOps\n                               : E3. AgentsOps</code></pre> <p> </p>"},{"location":"mlpaths_grids/#a-general-data-science","title":"A. General Data Science","text":"<ul> <li> <p>  A1. Introduction to Data Science and Machine Learning</p> <p><p>Data Science is an interdisciplinary field focused on extracting knowledge and insights from data. Machine Learning (ML), a key component of Artificial Intelligence (AI), enables systems to learn from data to make decisions or predictions.</p></p> </li> <li> <p>  A2. Data Analysis with Pandas</p> <p><p>Pandas is an open-source Python library used for data manipulation and analysis. It provides data structures, such as Series (1D) and DataFrames (2D), designed to handle tabular datasets efficiently. </p> <li> <p>  A3. Data Visualization with Matplotlib and Seaborn</p> <p><p>Matplotlib is a library in Python that enables users to generate visualizations like histograms, scatter plots, bar charts, pie charts and much more. Seaborn is a visualization library that is built on top of Matplotlib. It provides data visualizations that are typically more aesthetic and statistically sophisticated.</p> <li> <p>  A4. Ethical Considerations of Data Science</p> <p><p>Ethics in data science encompasses the moral principles and guidelines that govern the collection, analysis, and use of data to ensure responsible and beneficial outcomes. </p>"},{"location":"mlpaths_grids/#b-statistics","title":"B. Statistics","text":"<ul> <li> <p>  B1. Descriptive Statistics</p> <p><p>Descriptive Statistics is a set of brief descriptive coefficients that summarize a given data set representative of an entire or sample population.</p> <li> <p>  B2. Probability Distributions</p> <p><p>In probability theory and statistics, a probability distribution is a function that gives the probabilities of occurrence of possible events for an experiment.</p> <li> <p>  B3. Inferential Statistics</p> <p><p>Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.</p> <li> <p>  B4. Bayesian Statistics</p> <p><p>Bayesian statistics is a method of statistical inference that uses Bayes' Theorem to update the probability of a hypothesis as new evidence becomes available. </p>"},{"location":"mlpaths_grids/#c-machine-learning","title":"C. Machine Learning","text":"<ul> <li> <p>  C1. Machine Learning with Scikit-Learn</p> <p><p>Scikit-learn is a powerful and widely used Python library for machine learning. </p> <li> <p>  C2. Unsupervised Learning</p> <p><p>Unsupervised learning is a type of machine learning where algorithms learn from unlabeled data, identifying patterns and structures without specific guidance or desired outputs.</p> <li> <p>  C3. Supervised Learning</p> <p><p>Supervised learning is a type of machine learning where an algorithm learns to predict an output variable by being trained on a labeled dataset. </p> <li> <p>  C4. Ensemble Learning</p> <p><p>Ensemble learning in machine learning combines multiple individual models (base learners) to create a more accurate and robust predictive model than any single model alone. </p>"},{"location":"mlpaths_grids/#d-deep-learning","title":"D. Deep Learning","text":"<ul> <li> <p>  D1. Deep Learning in PyTorch </p> <p><p>PyTorch is an open-source ML framework offering flexible deep learning development with Python integration. It features dynamic computation graphs and GPU acceleration for neural networks, computer vision, and NLP tasks.</p> <li> <p>  D2. Transformers with HuggingFace</p> <p><p>Hugging Face Transformers is a Python library and open-source framework used to access and utilize pre-trained machine learning models for tasks like natural language processing (NLP), computer vision, audio processing, and multi-modal applications. </p> <li> <p>   D3. Generative AI 1 - LLM, RAG</p> <p><p>Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. </p> <li> <p>  D4. Generative AI - Multimodal LLMs </p> <p><p>Multimodal LLMs, are advanced AI systems that can process and generate content across multiple types of data, or modalities, such as text, images, audio, and video. </p>"},{"location":"mlpaths_grids/#e-continuous-integration-continuous-development","title":"E. Continuous Integration / Continuous Development","text":"<ul> <li> <p>  D1. MLOps</p> <p><p>MLOps (Machine Learning Operations), is a way to manage machine learning models, making it easier to develop, deploy, and update them as business needs change.</p> <li> <p>  D2. LLMOps</p> <p><p> LLMOps (Large Language Model Operations), extends MLOps practices to handle large language model deployment challenges. It focuses on managing computational resources, prompt engineering, and monitoring model performance and ethics.</p> <li> <p>  D3. AgentOps</p> <p><p>AgentOps deploys autonomous agents that perform complex tasks independently. These agents work with APIs, use real-time data for decisions, and adapt dynamically - making them suitable for autonomous high-stakes applications.</p> <p>Created: 05/25/2025 (C. Liz\u00e1rraga); Updated: 05/29/2025 (C. Liz\u00e1rraga)</p> <p>  2025. University of Arizona DataLab, Data Science Institute</p>"},{"location":"phire/","title":"PHIRE","text":""},{"location":"phire/#university-of-arizona-phire-program","title":"University of Arizona PHIRE Program","text":""},{"location":"phire/#training-materials-2024","title":"Training materials 2024","text":"<ul> <li>Introduction to R</li> <li>Reproducible Research</li> <li>AI Tools Landscape</li> <li>Introduction to Git</li> <li>Introduction to Command Line Interface</li> <li>Basic Prompt Engineering</li> </ul>"},{"location":"capstones/readme/","title":"UofA DataLab Capstone Projects Delivered","text":"<p>(Image Credit: Priscilla Du Preez. Unsplash.com)</p>"},{"location":"capstones/readme/#spring-2025","title":"Spring 2025","text":""},{"location":"capstones/readme/#deep-learning-pose-estimation","title":"Deep Learning Pose Estimation.","text":"<p>Tracking animal behavior noninvasively is crucial for many scientific fields. Extracting animal poses without markers is essential in biomechanics, genetics, ethology, and neuroscience but challenging in dynamic backgrounds. Mentors: Nirav Merchant, Carlos Liz\u00e1rraga, Michele Cosi. Contributors: Adrian Girone, Nick Ferrante.</p> <ul> <li>GitHub Repo</li> <li>Poster</li> </ul>"},{"location":"capstones/readme/#pubmed-central-rag-agent-chatbot","title":"PubMed Central RAG Agent Chatbot","text":"<p>Agent-based Retrieval Augmented Generation of biological sciences research using LLMs. The outcome of this project will be a conversational chatbot able to retrieve and respond questions from an expansive database created with hundeds of thousands of papers. Mentor: Enrique Noriega. Contributors: Abhay Nandiraju, Abhishek Kumar, Dhawal Ajay Gajwe, and Syed Junaid Hussain.</p> <ul> <li>GitHub Repo</li> <li>Poster</li> <li>Report</li> </ul>"},{"location":"capstones/readme/#analyzing-how-effective-the-2023-rule-changes-were-to-mlb-viewership-attendance","title":"Analyzing how effective the 2023 rule changes were to MLB viewership &amp; attendance.","text":"<p>The project analyzes MLB attendance data (2001-2024) to assess how 2023 rule changes affected fan engagement, excluding pandemic-affected 2020-2021 seasons due to attendance restriction. Mentor: Carlos Liz\u00e1rraga. Contributors: Alvaro Borbon, Alec Fernandez, and Marissa Ronquillo. </p> <ul> <li>Poster</li> <li>Report</li> </ul>"},{"location":"capstones/readme/#other-projects","title":"Other projects","text":"<ul> <li>BASIS Charter School Senior Project: ML Weather Forecasting.  </li> </ul> <p>Updated 05/15/2025</p>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/","title":"A1 Intro to DataScience and ML","text":"<p> [Back to ML Paths Home!]</p>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#introduction-to-data-science-and-machine-learning","title":"Introduction to Data Science and Machine Learning \ud83e\udded\ud83e\udd16","text":"<p>(Image credit: Jason Leung. Unsplash.com)</p> <p>This module serves as your foundational entry point into the dynamic worlds of Data Science and Machine Learning. It's designed for self-paced, experiential learning, encouraging active engagement with Python and AI tools to progressively build your skills from foundational knowledge to analytical application.</p>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#1-general-learning-objectives","title":"1. General Learning Objectives","text":"<p>Upon successful completion of this module, you will be able to:</p> <ol> <li>Remember key terminology and foundational concepts of Data Science and Machine Learning. (Bloom's: Remember)</li> <li>Understand the typical workflow of a data science project and the roles of various components. (Bloom's: Understand)</li> <li>Understand the distinctions and relationships between Artificial Intelligence, Machine Learning, and Data Science. (Bloom's: Understand)</li> <li>Apply basic Python programming skills using essential libraries (Pandas, NumPy) for fundamental data loading, inspection, and manipulation tasks. (Bloom's: Apply)</li> <li>Apply core data visualization techniques using Matplotlib and Seaborn to explore datasets and communicate initial findings. (Bloom's: Apply)</li> <li>Apply the principles of a basic machine learning algorithm (e.g., k-Nearest Neighbors) to a simple classification problem using Scikit-learn. (Bloom's: Apply)</li> <li>Analyze simple datasets to identify appropriate questions, necessary preprocessing steps, and suitable introductory modeling approaches. (Bloom's: Analyze)</li> <li>Analyze the output and performance of a basic machine learning model to draw initial conclusions. (Bloom's: Analyze)</li> </ol> <p>Throughout this module, you are encouraged to use open-source Large Language Models (LLMs) as a learning aid to clarify concepts, debug code, and explore topics more deeply, thereby enhancing your independent learning journey.</p>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#2-topic-overview","title":"2. Topic Overview","text":"<p>Data Science is an interdisciplinary field that extracts knowledge and insights from structured and unstructured data using scientific methods, processes, algorithms, and systems. Machine Learning (ML), a core component of Artificial Intelligence (AI), provides systems with the ability to automatically learn and improve from experience without being explicitly programmed. This module introduces you to the foundational principles that underpin these fields, the typical lifecycle of a data science project (from data acquisition to model deployment and interpretation), and the ethical considerations involved. You'll discover how Python, with its rich ecosystem of libraries, has become the de facto language for data science, enabling practitioners to tackle complex problems across various domains like healthcare, finance, and technology. Understanding these fundamentals is the first crucial step toward becoming a proficient data scientist.</p>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#3-open-source-python-libraries","title":"3. Open-Source Python Libraries","text":"<p>These libraries are workhorses in the field of Data Science:</p> <ul> <li>NumPy (Numerical Python):<ul> <li>Description: The fundamental package for numerical computation in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently.</li> <li>Practical Applications: Performing mathematical and logical operations on arrays, Fourier transforms, routines for shape manipulation, linear algebra, and random number generation. It forms the basis for most other scientific computing and data analysis libraries in Python.</li> </ul> </li> <li>Pandas:<ul> <li>Description: A powerful and flexible open-source data analysis and manipulation tool, built on top of NumPy. It provides expressive data structures like DataFrame and Series, designed to make working with \"relational\" or \"labeled\" data both easy and intuitive.</li> <li>Practical Applications: Data cleaning, data transformation (reshaping, merging, joining), data loading from various file formats (CSV, Excel, SQL databases), time-series analysis, and exploratory data analysis.</li> </ul> </li> <li>Matplotlib:<ul> <li>Description: A comprehensive library for creating static, animated, and interactive visualizations in Python. It provides a MATLAB-like interface and an object-oriented API for embedding plots into applications.</li> <li>Practical Applications: Generating a wide variety of plots like line plots, scatter plots, histograms, bar charts, and more, for data exploration, presentation, and publication.</li> </ul> </li> <li>Seaborn:<ul> <li>Description: A Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</li> <li>Practical Applications: Creating more sophisticated statistical plots with less code, such as heatmaps, violin plots, pair plots, and complex categorical plots. Excellent for highlighting relationships and distributions.</li> </ul> </li> <li>Scikit-learn (sklearn):<ul> <li>Description: A simple and efficient tool for data mining and data analysis. It features various classification, regression, clustering, dimensionality reduction, model selection, and preprocessing algorithms.</li> <li>Practical Applications: Implementing machine learning models, evaluating model performance, preparing data for modeling (e.g., feature scaling, encoding categorical variables).</li> </ul> </li> <li>Jupyter Notebook / JupyterLab:<ul> <li>Description: Web-based interactive computational environments that allow you to create and share documents containing live code, equations, visualizations, and narrative text.</li> <li>Practical Applications: Ideal for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. They support an iterative, exploratory approach to data science.</li> </ul> </li> </ul>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#4-core-skills-to-develop","title":"4. Core Skills to Develop","text":"<p>Engaging with this module will help you develop these specific skills:</p> <ol> <li>Conceptual Fluency: Articulating the definitions and relationships between key Data Science and Machine Learning terms.</li> <li>Data Acclimation: Loading, inspecting, and performing initial descriptive analysis on diverse datasets using Pandas.</li> <li>Visual Exploration: Generating and interpreting basic statistical plots to identify patterns, distributions, and relationships in data.</li> <li>Introductory Modeling: Implementing and evaluating a simple classification model using Scikit-learn.</li> <li>AI-Augmented Learning: Effectively using LLMs to clarify concepts, troubleshoot Python code, and explore problem-solving alternatives.</li> </ol>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#5-subtopics","title":"5. Subtopics","text":"<p>This module is structured around five key subtopics:</p> <ol> <li>The Data Science Ecosystem: Defining Data Science, AI, ML, and the typical project lifecycle.</li> <li>Python Environment &amp; Jupyter Mastery: Setting up your tools for effective data science work.</li> <li>Data Foundations with NumPy &amp; Pandas: Core techniques for data manipulation and preparation.</li> <li>Visual Insights with Matplotlib &amp; Seaborn: Fundamentals of exploratory data visualization.</li> <li>First Steps in Machine Learning with Scikit-learn: Introduction to predictive modeling.</li> </ol>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#6-experiential-use-cases","title":"6. Experiential Use Cases","text":"<p>For each use case, remember to leverage AI tools if you get stuck or want to dive deeper. For instance, you can ask an LLM: \"Explain this Python error message: [paste error]\", \"Generate a simple Python script to load a CSV using Pandas,\" or \"What are some common ways to handle missing data in a dataset?\"</p>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#subtopic-1-the-data-science-ecosystem","title":"Subtopic 1: The Data Science Ecosystem","text":"<ul> <li>Use Case 1.1: Defining the Domain<ul> <li>Problem Definition: You are tasked with explaining Data Science, Machine Learning (ML), and Artificial Intelligence (AI) to a non-technical colleague and illustrating how they relate.</li> <li>Learning Goal: Create a concise written explanation or a simple diagram that defines AI, ML, and Data Science, highlighting their overlaps and distinctions. (Bloom's: Understand)</li> <li>Python Tools: N/A for direct coding. Use a text editor or diagramming tool.</li> </ul> </li> <li>Use Case 1.2: Real-World ML Applications<ul> <li>Problem Definition: Identify how machine learning impacts everyday life or specific industries.</li> <li>Learning Goal: Research and list three distinct real-world applications of machine learning. For each, briefly describe the problem it solves and the likely type of data used. (Bloom's: Remember, Understand)</li> <li>Python Tools: N/A for direct coding. Use web search and a text editor.</li> </ul> </li> <li>Use Case 1.3: Mapping the Data Science Workflow<ul> <li>Problem Definition: You need to understand the standard process flow of a data science project.</li> <li>Learning Goal: Outline the key stages of a common data science workflow (e.g., CRISP-DM). For each stage, describe its main objective and list one example activity. (Bloom's: Remember, Understand)</li> <li>Python Tools: N/A for direct coding. Use a text editor or presentation software.</li> </ul> </li> </ul>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#subtopic-2-python-environment-jupyter-mastery","title":"Subtopic 2: Python Environment &amp; Jupyter Mastery","text":"<ul> <li>Use Case 2.1: Environment Setup<ul> <li>Problem Definition: Ensure you have a functional Python environment with all necessary data science libraries.</li> <li>Learning Goal: Install Anaconda (or Miniconda), create a virtual environment, and install NumPy, Pandas, Matplotlib, Seaborn, and Scikit-learn. Verify installations within a Jupyter Notebook by importing each library. (Bloom's: Apply)</li> <li>Python Tools: Anaconda/Miniconda, command line/terminal, Jupyter Notebook.</li> </ul> </li> <li>Use Case 2.2: Jupyter Notebook Navigation<ul> <li>Problem Definition: Become proficient with the Jupyter Notebook interface for interactive coding and documentation.</li> <li>Learning Goal: Create a new Jupyter Notebook. Practice creating and running code cells (e.g., variable assignments, simple calculations), using Markdown cells for titles and formatted text (bold, italics, lists), and saving your notebook. (Bloom's: Apply)</li> <li>Python Tools: Jupyter Notebook.</li> </ul> </li> <li>Use Case 2.3: AI for Code Comprehension<ul> <li>Problem Definition: You encounter a Python code snippet online for data loading but don't fully understand its syntax or logic.</li> <li>Learning Goal: Use an LLM to explain a provided short Python script (e.g., a script that loads a CSV with Pandas and prints basic info). Document the explanation and re-run the script with understanding. (Bloom's: Understand, Apply)</li> <li>Python Tools: Jupyter Notebook, access to an LLM.</li> </ul> </li> </ul>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#subtopic-3-data-foundations-with-numpy-pandas","title":"Subtopic 3: Data Foundations with NumPy &amp; Pandas","text":"<ul> <li>For these use cases, you can find simple CSV datasets online (e.g., student performance, basic retail sales) or ask an LLM to generate sample CSV data for you to save and use.</li> <li>Use Case 3.1: Data Loading &amp; Initial Inspection<ul> <li>Problem Definition: You are given a dataset in a CSV file and need to understand its basic structure and content.</li> <li>Learning Goal: Load the CSV file into a Pandas DataFrame. Use functions like <code>.head()</code>, <code>.tail()</code>, <code>.info()</code>, <code>.shape</code>, and <code>.describe()</code> to gather initial insights about the data (number of rows/columns, data types, missing values, summary statistics). (Bloom's: Apply)</li> <li>Python Tools: Pandas, Jupyter Notebook.</li> </ul> </li> <li>Use Case 3.2: Basic Data Cleaning<ul> <li>Problem Definition: The loaded dataset contains some missing values that could affect future analysis.</li> <li>Learning Goal: Identify columns with missing values. Apply a simple strategy to handle them (e.g., filling numerical NaNs with the column mean or median, or dropping rows/columns if appropriate, justifying your choice). (Bloom's: Apply, Analyze)</li> <li>Python Tools: Pandas, NumPy, Jupyter Notebook.</li> </ul> </li> <li>Use Case 3.3: Data Selection &amp; Filtering<ul> <li>Problem Definition: You need to extract specific subsets of your data for a targeted analysis (e.g., analyze data for a particular category or time period).</li> <li>Learning Goal: Select specific columns from the DataFrame. Filter rows based on one or more conditions (e.g., select all records where 'sales' &gt; 1000 and 'region' == 'North'). (Bloom's: Apply)</li> <li>Python Tools: Pandas, Jupyter Notebook.</li> </ul> </li> </ul>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#subtopic-4-visual-insights-with-matplotlib-seaborn","title":"Subtopic 4: Visual Insights with Matplotlib &amp; Seaborn","text":"<ul> <li>Use Case 4.1: Univariate Visualization<ul> <li>Problem Definition: Understand the distribution of individual variables in your dataset.</li> <li>Learning Goal: Create a histogram for a numerical column (e.g., 'age' or 'price') and a bar chart for a categorical column (e.g., 'product_category') to visualize their distributions. Interpret what these plots tell you. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Matplotlib, Seaborn, Pandas, Jupyter Notebook.</li> </ul> </li> <li>Use Case 4.2: Bivariate Visualization<ul> <li>Problem Definition: Explore potential relationships between pairs of variables.</li> <li>Learning Goal: Generate a scatter plot to visualize the relationship between two numerical variables (e.g., 'study_hours' vs. 'exam_score'). Create a box plot to compare the distribution of a numerical variable across different categories of a categorical variable (e.g., 'income' by 'education_level'). Discuss any observed patterns. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Matplotlib, Seaborn, Pandas, Jupyter Notebook.</li> </ul> </li> <li>Use Case 4.3: Customizing Visualizations for Clarity<ul> <li>Problem Definition: Your basic plots are created, but they lack titles, proper labels, and could be more visually appealing for presentation.</li> <li>Learning Goal: Take one of the plots created earlier and add a descriptive title, clear x-axis and y-axis labels, and change the color or style to improve its readability and aesthetic quality. You can ask an LLM for suggestions on \"how to make a Matplotlib plot more informative.\" (Bloom's: Apply)</li> <li>Python Tools: Matplotlib, Seaborn, Pandas, Jupyter Notebook.</li> </ul> </li> </ul>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#subtopic-5-first-steps-in-machine-learning-with-scikit-learn","title":"Subtopic 5: First Steps in Machine Learning with Scikit-learn","text":"<ul> <li>Use a simple, well-known dataset for this, like the Iris dataset, which can be loaded directly from Scikit-learn: <code>from sklearn.datasets import load_iris</code>.</li> <li>Use Case 5.1: Understanding Features and Target<ul> <li>Problem Definition: Before building a model, you must clearly define what you are trying to predict and what information will be used for prediction.</li> <li>Learning Goal: Load the Iris dataset. Identify and separate the features (X \u2013 e.g., sepal length, petal width) and the target variable (y \u2013 e.g., species of iris). Explain why this is a classification problem. (Bloom's: Understand, Apply)</li> <li>Python Tools: Scikit-learn, Pandas (optional, for DataFrame conversion), Jupyter Notebook.</li> </ul> </li> <li>Use Case 5.2: Training a Simple Classifier<ul> <li>Problem Definition: You need to train a machine learning model to learn patterns from the data.</li> <li>Learning Goal: Split the data into training and testing sets. Train a k-Nearest Neighbors (k-NN) classifier on the training portion of the Iris dataset. (Bloom's: Apply)</li> <li>Python Tools: Scikit-learn (<code>train_test_split</code>, <code>KNeighborsClassifier</code>), Jupyter Notebook.</li> </ul> </li> <li>Use Case 5.3: Making Predictions and Basic Evaluation<ul> <li>Problem Definition: Once trained, you need to assess how well your model performs on unseen data.</li> <li>Learning Goal: Use the trained k-NN model to make predictions on the test set. Calculate the accuracy of the model and interpret what this score means in the context of the problem. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Scikit-learn, Jupyter Notebook.</li> </ul> </li> </ul>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#7-assessment-quiz","title":"7. Assessment Quiz","text":"<p>This quiz helps you self-assess your understanding. Answers can be verified by reviewing module content or quick experimentation.</p> <ol> <li> <p>Which of the following best describes Machine Learning?     a)  The science of making computers perform tasks that require human intelligence.     b)  A field of study that gives computers the ability to learn without being explicitly programmed.     c)  The process of using computers to analyze large datasets only.     d)  The development of software that can reason and solve complex problems like humans. (Answer: (1) )</p> </li> <li> <p>Consider the following Python code using Pandas:     <pre><code>import pandas as pd\ndata = {'colA': [1, None, 3, 4], 'colB': [5, 6, 7, 8]}\ndf = pd.DataFrame(data)\ndf['colA'].fillna(df['colA'].mean(), inplace=True)\n# Assuming the mean of the non-missing values in colA is (1+3+4)/3 = 2.66...\n</code></pre>     What will be the value in <code>df['colA'][1]</code> after this code runs?     a)  None     b)  0     c)  Approximately 2.67     d)  6 (Answer: (2) )</p> </li> <li> <p>Which Python library is primarily used for creating statistical visualizations like heatmaps and pair plots with concise syntax?     a)  NumPy     b)  Seaborn     c)  Pandas     d)  Scikit-learn (Answer: (1) )</p> </li> <li> <p>In a typical classification problem, what is the role of the 'target variable'?     a)  It's an input feature used by the model to learn.     b)  It's the categorical label or class that the model aims to predict.     c)  It's a numerical value the model tries to estimate.     d)  It's a technique for reducing the number of features. (Answer: (1) )</p> </li> <li> <p>What is the primary purpose of <code>train_test_split</code> in Scikit-learn?     a)  To combine two different datasets into one.     b)  To separate features from the target variable within a single dataset.     c)  To divide a dataset into one part for training the model and another, unseen part for evaluating its performance.     d)  To visualize the distribution of data. (Answer: (2) )</p> </li> <li> <p>If you want to create a scatter plot in Python to visualize the relationship between 'Height' and 'Weight' columns in a Pandas DataFrame <code>df</code>, which line of code is most appropriate using Seaborn?     a)  <code>sns.histplot(data=df, x='Height', y='Weight')</code>     b)  <code>sns.boxplot(data=df, x='Height', y='Weight')</code>     c)  <code>sns.scatterplot(data=df, x='Height', y='Weight')</code>     d)  <code>df.plot(kind='scatter', x='Height', y='Weight')</code> (This is Pandas plotting, not Seaborn directly) (Answer: (2) )</p> </li> <li> <p>You have loaded a dataset into a Pandas DataFrame called <code>sales_df</code>. How would you display the first 10 rows of this DataFrame?     a)  <code>sales_df.show(10)</code>     b)  <code>sales_df.display_head(10)</code>     c)  <code>sales_df.head(10)</code>     d)  <code>sales_df.first(10)</code> (Answer: (2) )</p> </li> <li> <p>When you encounter a Python error message that you don't understand while working in a Jupyter Notebook, how can an LLM assist you most effectively?     a)  By automatically fixing the code in your notebook.     b)  By explaining what the error message typically means, suggesting possible causes, and providing examples of how to fix similar errors.     c)  By providing a link to the full Python documentation without context.     d)  By advising you to restart your computer. (Answer: (1) )</p> </li> <li> <p>What does the <code>.info()</code> method in Pandas primarily provide for a DataFrame?     a)  A statistical summary of numerical columns (mean, std, min, max).     b)  The first five rows of the DataFrame.     c)  A concise summary of the DataFrame, including data types of columns and non-null counts.     d)  The correlation matrix of numerical columns. (Answer: (2) )</p> </li> <li> <p>Which of these tasks falls under the 'Data Cleaning/Preparation' stage of the data science workflow?     a)  Defining business objectives.     b)  Training a machine learning model.     c)  Handling missing values and transforming variables.     d)  Presenting results to stakeholders. (Answer: (2) )</p> </li> </ol> <ol> <li>Answer is: b</li> <li>Answer is: c </li> </ol>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#8-bonus-challenge-problems","title":"8. Bonus Challenge Problems","text":"<p>These challenges encourage you to synthesize your learning and explore concepts at a deeper analytical level.</p> <ol> <li> <p>Mini-Project: Exploratory Data Analysis (EDA) on a Novel Dataset (Bloom's: Analyze)</p> <ul> <li>Problem: Find a small, interesting, and publicly available dataset (e.g., from Kaggle Datasets, UCI Machine Learning Repository - look for simpler ones). Perform a basic EDA. This should include:<ul> <li>Loading the dataset.</li> <li>Inspecting its structure, data types, and identifying missing values.</li> <li>Formulating at least three initial questions about the data that you find interesting.</li> <li>Creating at least three distinct types of visualizations to help answer your questions or explore patterns.</li> <li>Writing a brief summary of your findings and any challenges encountered.</li> </ul> </li> <li>Guidance: Document your steps and reasoning in a Jupyter Notebook. Use LLMs to help you find datasets or to get ideas for relevant questions and visualizations for the dataset you choose. For example, \"Suggest interesting questions I can explore in a dataset about [dataset topic].\"</li> </ul> </li> <li> <p>Comparing Classifiers: k-NN vs. Another (Bloom's: Analyze, Evaluate - introductory level)</p> <ul> <li>Problem: Using the Iris dataset (or another simple classification dataset you find), train and evaluate the k-Nearest Neighbors (k-NN) classifier as done in the module. Then, research and implement one other simple classification algorithm available in Scikit-learn (e.g., Logistic Regression or a Decision Tree).<ul> <li>Train this new model on the same training data.</li> <li>Evaluate it on the same test data using accuracy.</li> <li>Briefly compare the results. Which performed better on this specific task?</li> <li>Use an LLM to help you understand the basic principles of the new classifier you chose (\"Explain Logistic Regression in simple terms for a beginner\").</li> </ul> </li> <li>Guidance: Document your code and a short comparison of the models' performance and any observations in a Jupyter Notebook. This is not about finding the \"best\" model in an absolute sense, but about the process of applying and comparing.</li> </ul> </li> </ol>"},{"location":"mlpaths/A1_Intro_to_DataScience_and_ML/#9-references-further-reading","title":"9. References &amp; Further Reading","text":"<p>(Focus on open-access or widely available resources)</p> <ol> <li>VanderPlas, J. (2016). Python Data Science Handbook. O'Reilly Media.<ul> <li>Open-access online version: https://jakevdp.github.io/PythonDataScienceHandbook/</li> <li>A comprehensive guide covering IPython, NumPy, Pandas, Matplotlib, and Scikit-learn. Excellent for practical learning.</li> </ul> </li> <li>Grus, J. (2019). Data Science from Scratch: First Principles with Python (2<sup>nd</sup> ed.). O'Reilly Media.<ul> <li>While not fully open-access, many concepts are foundational and widely discussed. Focuses on understanding by building from scratch.</li> </ul> </li> <li>Scikit-learn User Guide.<ul> <li>Open-access: https://scikit-learn.org/stable/user_guide.html</li> <li>The official documentation is extensive, with tutorials and examples for all its modules.</li> </ul> </li> <li>Pandas Documentation: Getting Started &amp; User Guide.<ul> <li>Open-access: https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html</li> <li>Authoritative source for learning Pandas, from basic to advanced features.</li> </ul> </li> <li>StatQuest with Josh Starmer.<ul> <li>Open-access (YouTube): https://www.youtube.com/user/joshstarmer</li> <li>Provides clear and intuitive explanations of key statistics, machine learning, and data science concepts. Highly recommended for understanding the \"why\" behind the techniques.</li> </ul> </li> </ol> <p>Created: 05/25/2025 (C. Liz\u00e1rraga); Updated: 05/25/2025 (C. Liz\u00e1rraga)</p> <p> 2025. University of Arizona DataLab, Data Science Institute</p>"},{"location":"mlpaths/A2_Python_for_DataScience/","title":"A2 Python for DataScience","text":"<p> [Back to ML Paths]</p>"},{"location":"mlpaths/A2_Python_for_DataScience/#self-directed-learning-module-python-for-data-science","title":"Self-Directed Learning Module: Python for Data Science \ud83d\udc0d\ud83d\udd2c","text":"<p>(Image Credit: Trac Vu. Unsplash.com)</p> <p>This module focuses on mastering Python for the critical stages of the data science workflow that precede machine learning. You'll delve into data acquisition, cleaning, manipulation, analysis, and visualization using Python's powerful ecosystem. This experiential, self-paced program emphasizes hands-on learning and the integration of AI tools to support your development.</p>"},{"location":"mlpaths/A2_Python_for_DataScience/#1-general-learning-objectives","title":"1. General Learning Objectives","text":"<p>Upon successful completion of this module, you will be able to:</p> <ol> <li>Remember core Python syntax, data structures, and control flow relevant to data manipulation tasks. (Bloom's: Remember)</li> <li>Understand the principles of data wrangling, cleaning, and the importance of data quality. (Bloom's: Understand)</li> <li>Understand how to use NumPy for efficient numerical operations and array manipulations in data preparation. (Bloom's: Understand)</li> <li>Apply Pandas library functions to effectively load, inspect, clean, transform, merge, and aggregate diverse datasets. (Bloom's: Apply)</li> <li>Apply Matplotlib and Seaborn to create a variety of informative visualizations for exploratory data analysis and communicating findings. (Bloom's: Apply)</li> <li>Analyze datasets to identify data quality issues, inconsistencies, and patterns through programmatic inspection and visualization. (Bloom's: Analyze)</li> <li>Analyze and select appropriate Python techniques and tools for specific data manipulation and analysis challenges. (Bloom's: Analyze)</li> </ol> <p>Throughout this module, you are encouraged to use open-source Large Language Models (LLMs) as a learning aid to clarify Pythonic practices, debug code, generate example snippets, and explore data handling strategies more deeply.</p>"},{"location":"mlpaths/A2_Python_for_DataScience/#2-topic-overview","title":"2. Topic Overview","text":"<p>Effective data science relies heavily on robust data preparation and exploration, stages where Python excels. This module immerses you in using Python for the entire pre-modeling pipeline: acquiring data from various sources, performing intensive cleaning and transformation, conducting thorough exploratory data analysis (EDA) to uncover insights, and creating compelling visualizations to communicate findings. We will focus on writing efficient, readable, and \"Pythonic\" code using libraries like Pandas, NumPy, Matplotlib, and Seaborn. Mastering these skills is fundamental for any data scientist, as the quality of data preparation directly impacts the success of any subsequent analytical or modeling efforts. This module specifically excludes machine learning algorithms to allow for a deeper dive into these foundational data handling capabilities.</p>"},{"location":"mlpaths/A2_Python_for_DataScience/#3-open-source-python-libraries","title":"3. Open-Source Python Libraries","text":"<p>These libraries are essential for data handling and analysis in Python:</p> <ul> <li>Python Standard Library (selected modules):<ul> <li>Description: Python's built-in modules provide foundational capabilities. <code>csv</code> for reading/writing CSV files, <code>json</code> for handling JSON data, <code>collections</code> for specialized data structures (e.g., <code>Counter</code>, <code>defaultdict</code>), and <code>datetime</code> for date/time manipulation are particularly relevant.</li> <li>Practical Applications: Basic file I/O, parsing simple data formats, efficient counting, and managing temporal data without external dependencies.</li> </ul> </li> <li>NumPy (Numerical Python):<ul> <li>Description: The cornerstone for numerical computing in Python. It introduces the powerful N-dimensional array (<code>ndarray</code>) object, enabling efficient vectorized operations, which are significantly faster than Python's native loops for numerical tasks.</li> <li>Practical Applications: Performing complex mathematical calculations, statistical operations, linear algebra, random number generation, and serving as the underlying data structure for many other data science libraries like Pandas.</li> </ul> </li> <li>Pandas:<ul> <li>Description: A high-performance, easy-to-use library providing versatile data structures (like DataFrame and Series) and a rich set of tools for data manipulation and analysis. It's designed for working with structured (tabular, relational) and time-series data.</li> <li>Practical Applications: Data loading (from CSV, Excel, SQL, JSON, etc.), data cleaning (handling missing values, duplicates, outliers), data transformation (merging, joining, reshaping, pivoting, applying functions), filtering, grouping, and aggregation.</li> </ul> </li> <li>Matplotlib:<ul> <li>Description: A foundational and widely used plotting library in Python that provides extensive control over all aspects of a figure. It can produce a vast array of static, animated, and interactive visualizations.</li> <li>Practical Applications: Creating line charts, bar charts, histograms, scatter plots, pie charts, error bars, and more. Essential for detailed data exploration and producing publication-quality figures.</li> </ul> </li> <li>Seaborn:<ul> <li>Description: Built on top of Matplotlib, Seaborn provides a higher-level interface for creating attractive and informative statistical graphics. It simplifies the creation of common complex visualizations.</li> <li>Practical Applications: Generating statistical plots like distribution plots, categorical plots (box plots, violin plots), regression plots, heatmaps, and pair plots with concise syntax, often enhancing EDA.</li> </ul> </li> <li>Requests:<ul> <li>Description: A simple yet elegant HTTP library for Python, making HTTP requests straightforward.</li> <li>Practical Applications: Accessing web APIs to fetch data (e.g., JSON data from a REST API) for your data science projects.</li> </ul> </li> </ul>"},{"location":"mlpaths/A2_Python_for_DataScience/#4-core-skills-to-develop","title":"4. Core Skills to Develop","text":"<p>Engaging with this module will help you develop these specific skills:</p> <ol> <li>Pythonic Data Manipulation: Writing efficient, idiomatic Python code for complex data transformation and cleaning tasks.</li> <li>Advanced Data Wrangling: Mastering techniques for handling missing data, transforming data types, merging datasets, and reshaping data structures using Pandas.</li> <li>In-depth Exploratory Data Analysis (EDA): Systematically exploring datasets to uncover patterns, anomalies, and relationships using statistical summaries and diverse visualizations.</li> <li>Data Input/Output Expertise: Confidently reading data from and writing data to various common file formats (CSV, JSON, Excel).</li> <li>AI-Assisted Python Development: Utilizing LLMs to optimize Python code, understand library functionalities, and troubleshoot data-specific programming challenges.</li> </ol>"},{"location":"mlpaths/A2_Python_for_DataScience/#5-subtopics","title":"5. Subtopics","text":"<p>This module is structured around five key subtopics, focusing on Python for data work before modeling:</p> <ol> <li>Pythonic Foundations for Data Work: Revisiting essential Python data structures, functions, control flow, and list/dictionary comprehensions with a data-centric approach.</li> <li>Numerical Data Operations with NumPy: In-depth array manipulation, broadcasting, vectorized computations, and statistical functions for data preparation.</li> <li>Comprehensive Data Acquisition &amp; Cleaning with Pandas: Advanced techniques for reading diverse data formats, identifying and handling missing/erroneous data, and data type conversions.</li> <li>Transforming and Reshaping Data with Pandas: Mastering merging, joining, concatenating, pivoting, and applying custom functions to DataFrames.</li> <li>Exploratory Data Analysis and Visualization: Deep dive into using Pandas, Matplotlib, and Seaborn for statistical exploration and creating insightful visualizations.</li> </ol>"},{"location":"mlpaths/A2_Python_for_DataScience/#6-experiential-use-cases","title":"6. Experiential Use Cases","text":"<p>Remember to consult AI tools (like a locally run LLM or an online one) to help you understand error messages, suggest alternative Pythonic solutions, or explain library functions in more detail. For example: \"Explain how Pandas <code>merge</code> differs from <code>concat</code> with examples,\" or \"Show me a Pythonic way to apply a custom function to each row of a Pandas DataFrame.\"</p>"},{"location":"mlpaths/A2_Python_for_DataScience/#subtopic-1-pythonic-foundations-for-data-work","title":"Subtopic 1: Pythonic Foundations for Data Work","text":"<ul> <li>Use Case 1.1: Data Aggregation with Dictionaries and Lists<ul> <li>Problem Definition: You have a list of sales transaction tuples <code>(product_id, quantity_sold, price_per_unit)</code>. You need to calculate the total revenue for each <code>product_id</code>.</li> <li>Learning Goal: Write Python code using dictionaries to aggregate total revenue per product. Practice iterating through lists and updating dictionary values. (Bloom's: Apply)</li> <li>Python Tools: Python lists, dictionaries, <code>for</code> loops, <code>if</code> statements.</li> </ul> </li> <li>Use Case 1.2: Filtering Complex Data with Comprehensions<ul> <li>Problem Definition: Given a list of dictionaries, where each dictionary represents a student with keys like 'name', 'grade', and 'activities' (a list of strings), filter this list to get only students who are in 'grade' 10 and participate in 'chess club'.</li> <li>Learning Goal: Implement this filtering logic using a list comprehension with nested conditions or chained comprehensions. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Python lists, dictionaries, list comprehensions.</li> </ul> </li> <li>Use Case 1.3: Writing a Reusable Data Processing Function<ul> <li>Problem Definition: You frequently need to clean text data by converting it to lowercase, removing leading/trailing whitespace, and replacing multiple spaces with a single space.</li> <li>Learning Goal: Create a Python function that takes a string as input and returns the cleaned string according to the specified rules. Include a docstring for your function. (Bloom's: Apply)</li> <li>Python Tools: Python functions (<code>def</code>), string methods (<code>lower()</code>, <code>strip()</code>, <code>split()</code>, <code>join()</code>).</li> </ul> </li> </ul>"},{"location":"mlpaths/A2_Python_for_DataScience/#subtopic-2-numerical-data-operations-with-numpy","title":"Subtopic 2: Numerical Data Operations with NumPy","text":"<ul> <li>Use Case 2.1: Creating and Inspecting NumPy Arrays<ul> <li>Problem Definition: You need to represent a small table of numerical sensor readings (e.g., 5 readings for 3 different sensors) and perform basic inspections.</li> <li>Learning Goal: Create a 2D NumPy array from a Python list of lists. Inspect its <code>shape</code>, <code>dtype</code>, <code>ndim</code>, and <code>size</code> attributes. (Bloom's: Apply)</li> <li>Python Tools: NumPy (<code>np.array</code>, array attributes).</li> </ul> </li> <li>Use Case 2.2: Vectorized Arithmetic and Broadcasting<ul> <li>Problem Definition: You have a NumPy array representing product prices in USD. You need to convert all prices to EUR using a fixed exchange rate and then add a flat shipping fee to each.</li> <li>Learning Goal: Perform these calculations using vectorized operations (multiplication by exchange rate, addition of shipping fee) demonstrating NumPy's broadcasting. (Bloom's: Apply, Understand)</li> <li>Python Tools: NumPy arrays, arithmetic operations.</li> </ul> </li> <li>Use Case 2.3: Conditional Selection and Modification with NumPy<ul> <li>Problem Definition: Given a NumPy array of temperature readings, you need to identify all readings below a certain threshold (e.g., freezing point) and replace them with a specific value (e.g., the threshold itself, or NaN).</li> <li>Learning Goal: Use boolean indexing to select elements that meet a condition and then modify these selected elements. (Bloom's: Apply, Analyze)</li> <li>Python Tools: NumPy arrays, boolean indexing, <code>np.nan</code>.</li> </ul> </li> </ul>"},{"location":"mlpaths/A2_Python_for_DataScience/#subtopic-3-comprehensive-data-acquisition-cleaning-with-pandas","title":"Subtopic 3: Comprehensive Data Acquisition &amp; Cleaning with Pandas","text":"<ul> <li>Use Case 3.1: Loading and Inspecting Diverse File Formats<ul> <li>Problem Definition: You receive data from two sources: a CSV file containing product information and a JSON file containing customer reviews.</li> <li>Learning Goal: Load the CSV data into one Pandas DataFrame and the JSON data into another. Perform initial inspection (<code>.head()</code>, <code>.info()</code>, <code>.describe()</code>) on both to understand their structure and data types. (Bloom's: Apply)</li> <li>Python Tools: Pandas (<code>pd.read_csv()</code>, <code>pd.read_json()</code>).</li> </ul> </li> <li>Use Case 3.2: Advanced Missing Data Imputation<ul> <li>Problem Definition: A dataset of employee information has missing values in 'Salary' (numerical) and 'Department' (categorical) columns.</li> <li>Learning Goal: Implement different imputation strategies: fill missing 'Salary' values with the median salary of their respective 'JobLevel'. Fill missing 'Department' values using the mode or by forward/backward fill based on some criteria (e.g., if sorted by employee ID). Justify your choices. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Pandas (<code>.fillna()</code>, <code>.groupby()</code>, <code>.transform()</code>, <code>.median()</code>, <code>.mode()</code>, <code>ffill</code>, <code>bfill</code>).</li> </ul> </li> <li>Use Case 3.3: Identifying and Handling Duplicates and Inconsistent Text<ul> <li>Problem Definition: A customer dataset has duplicate entries based on email address and inconsistent entries in a 'Country' column (e.g., \"USA\", \"U.S.A.\", \"United States\").</li> <li>Learning Goal: Identify and remove duplicate rows, keeping the first occurrence. Standardize the 'Country' column to a consistent format (e.g., \"USA\"). (Bloom's: Apply, Analyze)</li> <li>Python Tools: Pandas (<code>.duplicated()</code>, <code>.drop_duplicates()</code>, string methods, <code>.replace()</code> or <code>.map()</code>).</li> </ul> </li> </ul>"},{"location":"mlpaths/A2_Python_for_DataScience/#subtopic-4-transforming-and-reshaping-data-with-pandas","title":"Subtopic 4: Transforming and Reshaping Data with Pandas","text":"<ul> <li>Use Case 4.1: Combining Datasets with Merging/Joining<ul> <li>Problem Definition: You have two DataFrames: one with order details (<code>order_id</code>, <code>customer_id</code>, <code>product_id</code>) and another with customer details (<code>customer_id</code>, <code>customer_name</code>, <code>city</code>). You need to combine them to get customer names and cities for each order.</li> <li>Learning Goal: Perform an appropriate merge (e.g., left merge) to combine these DataFrames based on <code>customer_id</code>. (Bloom's: Apply)</li> <li>Python Tools: Pandas (<code>pd.merge()</code>).</li> </ul> </li> <li>Use Case 4.2: Reshaping Data with Pivot Tables<ul> <li>Problem Definition: You have a \"long\" format DataFrame of sales data with columns <code>Date</code>, <code>ProductCategory</code>, <code>Region</code>, and <code>SalesAmount</code>. You need to transform it into a \"wide\" format where rows are <code>ProductCategory</code>, columns are <code>Region</code>, and cell values are total <code>SalesAmount</code>.</li> <li>Learning Goal: Use Pandas <code>pivot_table()</code> to reshape the data, calculating the sum of sales for each category-region combination. (Bloom's: Apply, Understand)</li> <li>Python Tools: Pandas (<code>.pivot_table()</code>).</li> </ul> </li> <li>Use Case 4.3: Applying Custom Functions for Complex Transformations<ul> <li>Problem Definition: You have a DataFrame with a 'transaction_date' column (as strings) and a 'price' column. You need to extract the day of the week from the date and create a new column indicating if the price is 'High', 'Medium', or 'Low' based on custom quantile-based thresholds.</li> <li>Learning Goal: Convert the 'transaction_date' to datetime objects. Write a custom function to categorize prices and apply it to the 'price' column using <code>.apply()</code> to create the new 'price_category' column. Extract the day of the week into a new column. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Pandas (<code>pd.to_datetime()</code>, <code>.dt.day_name()</code>, <code>.apply()</code>, custom Python functions, <code>.quantile()</code>).</li> </ul> </li> </ul>"},{"location":"mlpaths/A2_Python_for_DataScience/#subtopic-5-exploratory-data-analysis-and-visualization","title":"Subtopic 5: Exploratory Data Analysis and Visualization","text":"<ul> <li>Use Case 5.1: Univariate Analysis - Distributions and Outliers<ul> <li>Problem Definition: For a given numerical feature in your dataset (e.g., 'age' of customers, 'response_time' of a service), you need to understand its distribution, central tendency, and spread, and identify potential outliers.</li> <li>Learning Goal: Calculate descriptive statistics (mean, median, std, min, max, quartiles) for the feature. Create a histogram and a box plot to visualize its distribution and identify outliers. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Pandas (<code>.describe()</code>), Matplotlib/Seaborn (<code>histplot</code>, <code>boxplot</code>).</li> </ul> </li> <li>Use Case 5.2: Bivariate Analysis - Relationships and Correlations<ul> <li>Problem Definition: You want to investigate the relationship between two numerical variables (e.g., 'study_hours' and 'exam_score') and between a numerical and a categorical variable (e.g., 'income' across different 'education_levels').</li> <li>Learning Goal: Create a scatter plot for the two numerical variables and calculate their correlation coefficient. Create grouped box plots or violin plots for the numerical vs. categorical variable. Interpret the findings. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Pandas (<code>.corr()</code>), Matplotlib/Seaborn (<code>scatterplot</code>, <code>boxplot</code>, <code>violinplot</code>).</li> </ul> </li> <li>Use Case 5.3: Creating a Multi-plot Dashboard for Insights<ul> <li>Problem Definition: You need to present a consolidated view of several key aspects of a dataset to a stakeholder.</li> <li>Learning Goal: Create a figure with multiple subplots (e.g., a 2x2 grid) using Matplotlib, where each subplot shows a different insightful visualization (e.g., a distribution, a relationship, a comparison across categories) from your dataset. Ensure plots are well-labeled and titled. (Bloom's: Apply, Analyze)</li> <li>Python Tools: Matplotlib (<code>plt.subplots()</code>, <code>ax.set_title()</code>, etc.), Seaborn (can be used for individual plots within subplots).</li> </ul> </li> </ul>"},{"location":"mlpaths/A2_Python_for_DataScience/#7-assessment-quiz","title":"7. Assessment Quiz","text":"<p>This quiz helps you self-assess your understanding. Verify answers by reviewing module content or quick experimentation.</p> <ol> <li> <p>Which Python data structure is mutable and typically used for ordered sequences where elements can be changed?     a)  Tuple     b)  Set     c)  String     d)  List (Answer: d)</p> </li> <li> <p>What is the primary advantage of using NumPy's vectorized operations over standard Python loops for numerical computations?     a)  They require less memory.     b)  They are significantly faster due to optimized C implementations.     c)  They can handle non-numeric data more easily.     d)  They automatically generate plots. (Answer: b)</p> </li> <li> <p>In Pandas, if <code>df</code> is a DataFrame, what does <code>df.dropna(subset=['email'], inplace=True)</code> do?     a)  Drops rows where any column has a missing value.     b)  Drops rows where the 'email' column has a missing value, modifying <code>df</code> directly.     c)  Fills missing 'email' values with the mean.     d)  Drops the 'email' column itself. (Answer: b)</p> </li> <li> <p>Which Pandas method is most suitable for combining two DataFrames based on the values in one or more common columns (similar to SQL joins)?     a)  <code>pd.concat()</code>     b)  <code>df.append()</code>     c)  <code>pd.merge()</code>     d)  <code>df.join()</code> (Note: <code>join</code> is often index-based or can use <code>on</code> for columns, but <code>merge</code> is more general for column-based joining) (Answer: c)</p> </li> <li> <p>What does the Pandas <code>groupby()</code> method enable you to do?     a)  Sort the DataFrame by one or more columns.     b)  Split the DataFrame into groups based on some criteria, apply a function to each group independently, and then combine the results.     c)  Select a random sample of rows from the DataFrame.     d)  Reshape the DataFrame from long to wide format. (Answer: b)</p> </li> <li> <p>Which Matplotlib/Seaborn plot is best for visualizing the distribution of a single continuous numerical variable and checking for skewness or modality?     a)  Scatter plot     b)  Bar chart     c)  Histogram or Kernel Density Estimate (KDE) plot     d)  Line chart (Answer: c)</p> </li> <li> <p>If you have a Pandas DataFrame <code>sales_data</code> and want to find the average sales amount for each <code>product_category</code>, which code snippet is most appropriate?     a)  <code>sales_data.pivot_table(columns='product_category', values='sales_amount', aggfunc='mean')</code>     b)  <code>sales_data.groupby('product_category')['sales_amount'].mean()</code>     c)  <code>sales_data.describe()['sales_amount']</code>     d)  <code>sales_data['sales_amount'].apply(lambda x: x.mean() if x.name == 'product_category' else x)</code> (Answer: b)</p> </li> <li> <p>You've written a Python script to process a large CSV. It's running much slower than expected. How could an LLM potentially help you optimize it (without directly accessing your system)?     a)  By remotely debugging your script on your machine.     b.  By explaining common Python performance bottlenecks, suggesting more efficient Pandas/NumPy functions for tasks you describe (e.g., replacing row-wise iteration with vectorized operations), or helping refactor a specific slow code snippet you provide.     c.  By rewriting the entire script in C++ for you.     d.  By telling you to buy a faster computer. (Answer: b)</p> </li> <li> <p>What is the purpose of <code>df.info()</code> for a Pandas DataFrame <code>df</code>?     a)  To display the first 5 rows.     b)  To provide a summary of descriptive statistics.     c.  To show a concise summary including the index dtype and columns, non-null values, and memory usage.     d)  To check for duplicate rows. (Answer: c)</p> </li> <li> <p>Which Python library would you primarily use to fetch data from a public REST API that returns JSON?     a)  NumPy     b)  Pandas     c)  Requests (often combined with the <code>json</code> standard library module)     d)  Matplotlib (Answer: c)</p> </li> </ol>"},{"location":"mlpaths/A2_Python_for_DataScience/#8-bonus-challenge-problems","title":"8. Bonus Challenge Problems","text":"<p>These challenges are designed to push your analytical and Python data handling skills further.</p> <ol> <li> <p>Comprehensive Sales Data Analysis Pipeline (Bloom's: Analyze, Evaluate)</p> <ul> <li>Problem: Find a multi-file sales dataset (or generate a plausible one with an LLM, e.g., one file for transactions, one for product details, one for store locations). Your task is to create a full data pipeline in a Jupyter Notebook that:<ol> <li>Loads data from multiple sources.</li> <li>Cleans each dataset thoroughly (handle missing values, correct data types, standardize text, remove duplicates).</li> <li>Merges these datasets into a single analytical DataFrame.</li> <li>Performs EDA: Generate at least five distinct and insightful visualizations (e.g., sales trends over time, top-selling products, sales by region, correlation between product price and quantity sold). For each visualization, write a brief interpretation.</li> <li>Derive at least two new features that might be useful (e.g., profit margin if cost data is available/creatable, sales per square foot if store size is available/creatable, day of the week of sale).</li> <li>Summarize your key findings and any data quality issues or limitations you encountered.</li> </ol> </li> <li>Guidance: Justify your cleaning and transformation choices. Use LLMs to brainstorm potential new features or to get ideas for impactful visualizations given the (actual or mock) data. Focus on clear, well-commented code and insightful interpretations.</li> </ul> </li> <li> <p>Time-Series Data Anomaly Detection and Smoothing (Bloom's: Analyze, Evaluate)</p> <ul> <li>Problem: Obtain a simple time-series dataset (e.g., daily stock prices for a single stock over a year, monthly temperature readings for a city). Your task is to:<ol> <li>Load and preprocess the time-series data (ensure dates are proper datetime objects, handle missing values using time-series appropriate methods like interpolation).</li> <li>Visualize the raw time series.</li> <li>Implement a simple method to detect potential anomalies or outliers (e.g., values exceeding a certain rolling standard deviation, or using interquartile range on differences). Clearly mark these on your plot.</li> <li>Apply a smoothing technique (e.g., a simple moving average) to the time series and plot the smoothed series alongside the original.</li> <li>Discuss the effect of the smoothing technique and the rationale for your anomaly detection method. What are the pros and cons of the methods you chose?</li> </ol> </li> <li>Guidance: Pandas has excellent time-series capabilities (e.g., <code>resample()</code>, <code>rolling()</code>). Use an LLM to understand different smoothing techniques or simple anomaly detection approaches suitable for time series data if you're unfamiliar. Focus on interpretation and justifying your choices.</li> </ul> </li> </ol>"},{"location":"mlpaths/A2_Python_for_DataScience/#9-references-further-reading","title":"9. References &amp; Further Reading","text":"<p>(Focus on open-access or widely available resources)</p> <ol> <li>McKinney, W. (2022). Python for Data Analysis (3<sup>rd</sup> ed.). O'Reilly Media.<ul> <li>Authored by the creator of Pandas, this is the definitive guide for data manipulation with Pandas, also covering NumPy and practical data analysis techniques.</li> </ul> </li> <li>VanderPlas, J. (2016). Python Data Science Handbook. O'Reilly Media.<ul> <li>Open-access online version: https://jakevdp.github.io/PythonDataScienceHandbook/</li> <li>Excellent coverage of IPython, NumPy, Pandas, Matplotlib. Great for practical, hands-on learning.</li> </ul> </li> <li>Pandas Official Documentation: User Guide &amp; Tutorials.<ul> <li>Open-access: https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html</li> <li>The most authoritative and comprehensive resource for all Pandas functionalities.</li> </ul> </li> <li>NumPy Official Documentation: User Guide.<ul> <li>Open-access: https://numpy.org/doc/stable/user/index.html</li> <li>Essential for understanding NumPy arrays and their operations in depth.</li> </ul> </li> <li>Python Standard Library Documentation: <code>csv</code>, <code>json</code>, <code>collections</code>, <code>datetime</code>.<ul> <li>Open-access: https://docs.python.org/3/library/ (Navigate to specific modules)</li> <li>Direct from the source, for understanding Python's built-in capabilities for data handling.</li> </ul> </li> </ol> <p>Created: 05/25/2025 (C. Liz\u00e1rraga); Updated: 05/25/2025 (C. Liz\u00e1rraga)</p> <p> 2025. University of Arizona DataLab, Data Science Institute</p>"}]}